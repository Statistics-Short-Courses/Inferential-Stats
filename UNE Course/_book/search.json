[
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "2  Getting to Know Your Data",
    "section": "",
    "text": "2.1 Variables\nBefore we jump into the world of statistical testing, let’s take a moment to refresh our understanding of what a variable is, the different types of variables, and how we can describe them using numerical and graphical methods.\nSo what is a variable?\nThe Australian Bureau of Statistics defines a variable to be “any characteristic, number, or quantity that can be measured or counted.”\nIf you’ve ever tried dieting, you might have recorded your weight on your phone over time. Or maybe you’ve tracked how long your morning commute takes so you know when to leave to arrive on time. In both cases, you are measuring something that changes, and these are examples of variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to Know Your Data</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Welcome to your journey into Inferential Statistics!\nWhether you’re a research student hoping to build on your statistical skills or just starting your journey in the field, this course will give you the tools and confidence to apply inferential statistics in R effectively.\nIn this short course, we’ll explore a variety of essential concepts and techniques used in statistical analysis and apply these tools to real datasets and problems.\nWe’ll cover key background topics such as common statistical terminology (p-values, anyone?), types of variables, and more.\nAlong the way, you’ll build confidence in creating clear, visually appealing graphs, and in knowing when and how to use them appropriately.\nFinally, we will cover some useful and common statistical tests, such as:\n\nHypothesis testing\nt-tests (one-sample, two-sample, and paired)\nChi-squared tests (goodness of fit and independence)\nANOVA",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "chapter1.html#types-of-variables",
    "href": "chapter1.html#types-of-variables",
    "title": "2  Getting to Know Your Data",
    "section": "2.2 Types of Variables",
    "text": "2.2 Types of Variables\nThere are two main types of variables:\n\nNumerical\nCategorical\n\nA numerical variable represents values to describe a measurable quantity. A numerical variable can be either discrete or continuous.\nDiscrete numerical variables take on distinct whole values, and are countable. For example, if we were to look at the at the number of cars in a parking lot, the number of birds in a tree, or the number of times “3” is rolled in a series of die rolls, these are exact values that can be counted and are thus discrete.\nContinuous numerical variables can take on any value within a range of real numbers. In other words, they can be measured to any level of precision. For example, age exists on a continuous scale because, in theory, it can be measured infinitely precisely. Someone could be 10.25, 10.257, or even 10.2576326362 years old. In everyday life, we usually round age to the nearest year, but that does not change the fact that the underlying variable is continuous. Other common examples of continuous numerical variables include weight, height, and time. Can you explain why each of these variables is numerical and continuous?\nA categorical variable, on the other hand, represents categories or labels that describe a quality or characteristic. Categorical variables can be either nominal or ordinal.\nA nominal categorical variable includes observations with no particular order. For example, if you asked ten coworkers for their favorite movie and recorded their answers under a “movies” variable, it would be nominal because there is no inherent sequence or ranking among the movie titles.\nIn an ordinal categorical variable, there is an inherent logical order to the observations. For example, if we collected age groups such as “18–24”, “25–38”, “39–55”, “56–70”, and “70+”, we’re now working with labels that have a built-in order where “18–24” is younger than “25–38”, and so on.\nAnother example might be asking people whether they like ice cream, with options like “strongly agree”, “agree”, “neutral”, “disagree”, and “strongly disagree”. In this case, the order of the options has meaning, even if the distance between them isn’t equal.\nTake a moment to check your understanding with the exercises below.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to Know Your Data</span>"
    ]
  },
  {
    "objectID": "chapter1.html#exercise-1.1",
    "href": "chapter1.html#exercise-1.1",
    "title": "2  Chapter 1",
    "section": "2.3 Exercise 1.1",
    "text": "2.3 Exercise 1.1\n\nDescribe the following variables:\na. Eye colour is a discrete numerical variablecontinuous numerical variableordinal categorical variablenominal categorical variable\nb. Temperature is a discrete numerical variableordinal categorical variablenominal categorical variablecontinuous numerical variable\nc. The number of heads in a series of coin flips is a continuous numerical variableordinal categorical variablenominal categorical variablediscrete numerical variable\nd. Education level is a discrete numerical variablecontinuous numerical variablenominal categorical variableordinal categorical variable\ne. Postal Code is a discrete numerical variablecontinuous numerical variableordinal categorical variablenominal categorical variable\nf. Income salary is a discrete numerical variableordinal categorical variablenominal categorical variablecontinuous numerical variable\ng. Number of calls per day is a discrete numerical variablecontinuous numerical variableordinal categorical variablenominal categorical variablediscrete numerical variable",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter1.html#graphing",
    "href": "chapter1.html#graphing",
    "title": "2  Chapter 1",
    "section": "2.4 Graphing",
    "text": "2.4 Graphing\nNo",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter1.html#section",
    "href": "chapter1.html#section",
    "title": "2  Chapter 1",
    "section": "2.5 ",
    "text": "2.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UNE Course",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html#exercise-checkpoint-1",
    "href": "chapter1.html#exercise-checkpoint-1",
    "title": "2  Getting to Know Your Data",
    "section": "2.3 Exercise Checkpoint 1",
    "text": "2.3 Exercise Checkpoint 1\n\nDescribe the following variables:\na. Eye colour is a discrete numerical variablecontinuous numerical variableordinal categorical variablenominal categorical variable\nb. Temperature is a discrete numerical variableordinal categorical variablenominal categorical variablecontinuous numerical variable\nc. The number of heads in a series of coin flips is a continuous numerical variableordinal categorical variablenominal categorical variablediscrete numerical variable\nd. Education level is a discrete numerical variablecontinuous numerical variablenominal categorical variableordinal categorical variable\ne. Postal Code is a discrete numerical variablecontinuous numerical variableordinal categorical variablenominal categorical variable\nf. Income salary is a discrete numerical variableordinal categorical variablenominal categorical variablecontinuous numerical variable\ng. Number of calls per day is a discrete numerical variablecontinuous numerical variableordinal categorical variablenominal categorical variablediscrete numerical variable",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to Know Your Data</span>"
    ]
  },
  {
    "objectID": "chapter1.html#visually-representing-data",
    "href": "chapter1.html#visually-representing-data",
    "title": "2  Getting to Know Your Data",
    "section": "2.4 Visually Representing Data",
    "text": "2.4 Visually Representing Data\nNow that we’ve covered the main types of variables, we can start thinking about how to visually represent data.\nFirst, we will need to load in some data into R. We will be working with the palmerpenguins package. The palmerpenguins package is a dataset collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\nIf you have not used this package before, you will need to install it first:\n\ninstall.packages(\"palmerpenguins\")\n\nOnce installed, you will need to load the package into R.\n\nlibrary(palmerpenguins)\n\nNow that we have the package installed, let’s take a look at the variables in the dataset using str().\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nYou might notice that some values are listed as NA. This means there are missing observations. To check for missing values, we can use:\n\nany(is.na(penguins))\n\n[1] TRUE\n\n\nSince this returns TRUE, we know there are missing values in the dataset. Let’s find out how many:\n\nsum(is.na(penguins))\n\n[1] 19\n\n\nThere are 19 missing observations. This can cause problems when generating numerical summaries or running statistical tests. At this stage, to handle this, we can remove the missing values. It’s always best practice to store your cleaned data in a new dataset and leave the original untouched. Note that you should always be careful when removing observations, as even partially filled observations can still provide valuable information.\n\nclean_penguins &lt;- na.omit(penguins)\n\nWe can check the structure again to make sure everything looks good:\n\nstr(clean_penguins)\n\ntibble [333 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n $ bill_depth_mm    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n $ flipper_length_mm: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n $ body_mass_g      : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 2 1 2 2 ...\n $ year             : int [1:333] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:11] 4 9 10 11 12 48 179 219 257 269 ...\n  ..- attr(*, \"names\")= chr [1:11] \"4\" \"9\" \"10\" \"11\" ...\n\n\nWe can see that there are eight variables. Three of these are categorical: species, island, and sex. These are stored as factors, where the levels represent the number of distinct labels or groups within each variable. For example, sex has two levels: male and female.\nYou’ll notice that there are four numerical variables: bill_length_mm, bill_depth_mm, flipper_length_mm, and year. If your instinct is that year doesn’t quite fit as right as a numerical variable, you would be right! It is better to treat year as a categorical variable. To convert it, we can use:\n\nclean_penguins$year &lt;- as.factor(clean_penguins$year)\n\nNow if we run str again, we can see that year is now correctly stored as a categorical variable with three levels.\n\nstr(clean_penguins)\n\ntibble [333 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n $ bill_depth_mm    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n $ flipper_length_mm: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n $ body_mass_g      : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 2 1 2 2 ...\n $ year             : Factor w/ 3 levels \"2007\",\"2008\",..: 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:11] 4 9 10 11 12 48 179 219 257 269 ...\n  ..- attr(*, \"names\")= chr [1:11] \"4\" \"9\" \"10\" \"11\" ...\n\n\nIf you’re interested in seeing the unique groupings or labels within a categorical variable, you can use the unique() function.\n\nunique(clean_penguins$year)\n\n[1] 2007 2008 2009\nLevels: 2007 2008 2009\n\n\nThis shows that the data was collected over the years 2007, 2008, and 2009.\nYou can also see from the structure output str() that the penguins dataset is stored as a 333 x 8 tibble, which is a type of data frame in R. This means that there are 333 rows (observations) and 8 columns (variables).\nWe can confirm this using the dim() function.\n\ndim(clean_penguins)\n\n[1] 333   8\n\n\nIf you wish to have a closer look at inspecting the penguins dataset, you can use the View() function to open it in a new tab on RStudio.\n\nView(penguins)\nView(clean_penguins)\n\nNow that we’ve inspected the dataset, we can start thinking about how to visually represent and understand the data.\nWe can describe data using numerical summaries and graphs. Numerical summaries are statistical measures that capture key aspects of a dataset. The way we choose to present these summaries and the graphs that accompany them depends on the type of variable we are working with.\nLet’s start with numerical summaries.\nFor numerical variables, whether continuous or discrete, we can include:\n\nmean\nstandard deviation\nmedian\nminimum\nmaximum\nInterquartile Range (IQR)\n\nIMPORTANT NOTE: When reporting descriptive statistics, always provide appropriate context. If you report a mean, include the standard deviation, and vice versa. If you report a median, include the interquartile range (IQR), and vice versa. Likewise, if you report a maximum value, always include the minimum to give full perspective.\nSome helpful R functions for generating summary statistics:\n\n\n\n\n\n\n\n\nSummary Statistic\nCommand\n\n\n\n\nMean\nmean(...)\n\n\nStandard Deviation\nsd(...)\n\n\nMedian\nmedian(...)\n\n\nMinimum\nmin(...)\n\n\nMaximum\nmax(...)\n\n\nInterquartile Range\nIQR(...)\n\n\nFirst quartile\nquantile(..., 0.25)\n\n\nThird quartile\nquantile(..., 0.75)\n\n\nFive point summary\nsummary(...)\n\n\n\n\n\n\n\nLet’s take a look at calculating descriptive statistics for body_mass_g, which describes the body mass of each penguin in grams.\nNow we can attach the cleaned penguins dataset. Attaching the dataset allows us to call variables directly by name. If we don’t attach it, we can still access variables, but we need to use the full format (for example, clean_penguins$sex).\n\n# Attach cleaned dataset \nattach(clean_penguins)\n\n\n# Let's first calculate the mean and round it to one decimal place\nround(mean(body_mass_g),1)\n\n[1] 4207.1\n\n# Now we can calculate the standard deviation\nround(sd(body_mass_g),1)\n\n[1] 805.2\n\n\nThe average body mass of penguins is 4207.1 g, with a standard deviation of 805.2 g.\n\n# calculate the minimum\nmin(body_mass_g)\n\n[1] 2700\n\n# calculate the maximum\nmax(body_mass_g)\n\n[1] 6300\n\n\nThe minimum observed body mass is 2700 g, and the maximum observed body mass is 6300 g.\n\n# calculate median\nmedian(body_mass_g)\n\n[1] 4050\n\n# calculate IQR\n## first quantile\nquantile(body_mass_g, 0.25)\n\n 25% \n3550 \n\n## third quantile \nquantile(body_mass_g, 0.75)\n\n 75% \n4775 \n\n\nThe median body mass is 4050 g, with an interquartile range from 3350 g to 4775 g.\nThe summary() function lets us quickly gather all of this information at once.\n\n# Calculate 5-point summary\nsummary(body_mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2700    3550    4050    4207    4775    6300 \n\n\nNow that we’ve explored numerical summaries, let’s move on to graphing.\nWhen you have one numerical variable, whether discrete or continuous, a histogram is a great way to visualise its distribution.\nWe’ll use the ggplot2 package for this. If you haven’t used it before, you’ll need to install it first.\n\n# Install the package\ninstall.packages(ggplot2)\n\nOnce installed, you can load the package and start plotting.\n\n# Load the library\nlibrary(ggplot2)\n\n# We can plot the histogram\nggplot(clean_penguins, aes(x=body_mass_g))+\n  geom_histogram()\n\n\n\n\n\n\n\n\nThe histogram looks a bit plain without any color, and the x-axis label isn’t very informative. Let’s add some color, fix the labels, and give it a clean white background.\n\n# We can plot the histogram\nggplot(clean_penguins, aes(x=body_mass_g))+\n  geom_histogram(fill=\"orangered\", bins=30) +\n  labs(x=\"Body Mass of Penguins (g)\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nWhat if we’re interested in how body mass relates to flipper length? In this case, we’re working with two continuous numerical variables, so a scatterplot would be a good choice.\n\nggplot(clean_penguins, aes(x=body_mass_g, y=flipper_length_mm))+\n  geom_point()\n\n\n\n\n\n\n\n\nLike we did with the histogram, we can add color, change the shape of the points, and include informative labels.\n\nggplot(clean_penguins, aes(x=body_mass_g, y=flipper_length_mm))+\n  geom_point(fill=\"aquamarine\", col=\"aquamarine4\", pch=21, size=2.5)+\n  labs(x=\"Body Mass of Penguins (g)\", y=\"Flipper Length of Penguins (mm)\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can see that as the body mass of a penguin increases, its flipper length also tends to increase. This suggests a strong positive linear relationship.\nNow, let’s look at a case where we have one categorical variable and one numerical variable. A boxplot is a great choice here. Let’s see how body mass differs between penguin species.\n\nggplot(clean_penguins, aes(x=species, y=body_mass_g))+\n  geom_boxplot(fill = c(\"coral2\", \"chartreuse2\", \"cadetblue2\"),\n               col = c(\"coral3\", \"chartreuse4\", \"cadetblue4\")) +\n  labs(x=\"Species of Penguin\", y=\"Body Mass (g)\")\n\n\n\n\n\n\n\n\nWe can see that Gentoo penguins have a noticeably higher median body mass compared to Chinstrap and Adelie penguins. The interquartile range (the box) for the Gentoo species does not overlap with the others, suggesting there may be a significant difference between them.\nSince we’re now considering species, let’s also take a look at how many penguins were observed in each species. Because we’re dealing with a categorical variable, a bar plot is appropriate.\n\nggplot(clean_penguins, aes(x = species)) +\n  geom_bar(fill = c(\"coral2\", \"chartreuse2\", \"cadetblue2\"),\n               col = c(\"coral3\", \"chartreuse4\", \"cadetblue4\")) +\n  labs(x = \"Penguin Species\", y = \"Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can also explore proportions to understand how each species contributes to the total sample.\n\nggplot(data=clean_penguins, aes(x=species))+\n  geom_bar(fill = c(\"coral2\", \"chartreuse2\", \"cadetblue2\"),\n               col = c(\"coral3\", \"chartreuse4\", \"cadetblue4\"), \n           aes(y=after_stat(prop), group=1))+\n  labs(x=\"Penguin Species\", y=\"Proportion\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nAdelie penguins make up a much larger proportion of the sample, while Chinstrap penguins account for the smallest. It’s important to consider how such imbalances in your dataset could influence your interpretation.\nWe can also generate a mosaic plot for species. To do so, we will need to install ggmosaic.\n\ninstall.packages(\"ggmosaic\")\n\n\nlibrary(ggmosaic)\n\nggplot(data=clean_penguins, aes(col=species))+\n  geom_mosaic(aes(x=product(species)), fill=c(\"coral2\", \"chartreuse2\", \"cadetblue2\"))+\n  theme_bw() \n\n\n\n\n\n\n\n\nThe wider the tile, the greater the proportion that species contributes. Adelie has the widest tile, which confirms it makes up the largest share of the sample.\nWhat if we want to look at penguin species by the island where they were observed? We are now dealing with two categorical variables. Mosaic plots work well here too.\n\nggplot(clean_penguins) +\n  geom_mosaic(aes(x = product(island, species)), \n              fill = rep(c(\"coral2\", \"chartreuse2\", \"cadetblue2\"),3)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFrom this plot you can see:\n\nAdelie appear on Biscoe, Dream, and Torgersen in roughly similar counts.\nChinstrap are observed only on Dream.\nGentoo are observed only on Biscoe.\n\nThese patterns help explain why Adelie dominate overall, and they also show that species presence varies by island.\nBe sure to detach() your dataset before you move onto the exercises.\n\ndetach(clean_penguins)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to Know Your Data</span>"
    ]
  },
  {
    "objectID": "chapter1.html#exercise-checkpoint-2",
    "href": "chapter1.html#exercise-checkpoint-2",
    "title": "2  Getting to Know Your Data",
    "section": "2.5 Exercise Checkpoint 2",
    "text": "2.5 Exercise Checkpoint 2\n\nNow that you are more comfortable generating plots and summary statistics in R, it is a good time to test your skills on a new dataset, the built-in iris dataset. This dataset is already preloaded in RStudio, so there is no need to import anything.\nTo make sure it is available, you can view it by running\n\nView(iris)\n\nThis will open the dataset in a new tab in RStudio so you can explore its structure and variables before starting your analysis.\na. How many variables are in the iris dataset? \nb. How many observations are in the iris dataset? \nc. How many categorical variables are in the iris dataset? \nd. How many numerical variables are in the iris dataset? \ne. How many species are in the iris dataset? \nf. What are the names of the three species in the iris dataset? Please list them in alphabetical order, separated by commas (for example: x, y, z) \ng. Is Species nominal or ordinal? \nh. Is Sepal.Length continuous or discrete? \ni. What is the mean sepal width of iris varieties rounded to one decimal place? \nj. What is an appropriate plot to visualise petal width against species? \nk. What is an appropriate plot to visualise petal width against sepal width?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to Know Your Data</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Probability Distributions",
    "section": "",
    "text": "3.1 The Normal Distribution\nIn the previous chapter, we’ve described what we see in data. Understanding probability can help us reason about what we expect from our data.\nA random variable is a variable whose value is determined by chance. For example, if we are looking at the number of heads in a series of coin flips, then the number of heads is a random variable.\nA probability distribution is a model that describes the likelihood of each outcome. If we know what type of distribution to expect based on the kind of experiment or observation we are doing, we can model the probability or chance of those outcomes. In the case of counting the number of heads in a series of coin flips, this is known as a Bernoulli trial because each head can be counted as a success. The number of successes in several independent Bernoulli trials can be modeled by the binomial distribution. We will look at this in more detail later.\nA normal distribution is a continuous, symmetric, bell-shaped probability distribution defined by a mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). Normal distributions have:\nThis is also known as the 68-85-99.7 rule. This means that if you know the mean and the standard deviation of a normal data set you can calculate the range of values in which 68%, 95% and 99.7% of the data will fall.\nA standard normal distribution is a special case of the normal distribution that has a mean of 0 and a standard deviation of 1, which is denoted as \\(Z \\sim N(0,1)\\) where \\(Z\\) represents a standarised random variable . If the random variable is not represented by \\(Z\\) and is represented by \\(X\\) or any other letter, it is generally not a standardised normal variable. That is, \\(X \\sim (\\mu, \\sigma^2)\\). You can convert any normal variable \\(X\\) to a standard normal variable using:\n\\(Z=\\frac{X-\\mu}{\\sigma}\\)\nwhere \\(Z \\sim N(0,1)\\).\nWe might also want to find the probability of certain values occurring, which is represented by the area under the curve. For example, in a standard normal distribution, the probability that a value is greater than 2.5 (that is, \\(P(Z&gt;2.5)\\) . We can draw a graph and label the axes.\nWe know the mean is 0 and since the standard deviation is 1 that the curve approximately stretches between -3 and +3.\nNow draw in approximately where 2.5 would sit and shade in the correct part of the curve.\nLet’s work through a practical example.\nSuppose weights of the checked baggage of airline passengers follow a nearly normal distribution with mean 45 pounds and standard deviation 3.2 pounds. Most airlines charge a fee for baggage that weighs in excess of 50 pounds. How can this be expressed using probability notation P()?\nWhat is the mean?\nWhat is the standard deviation?\nWhat are the values that are 1, 2, and 3 standard deviations above the mean, in that order? Write your answers separated by commas (for example: 1, 2, 3).\nWhat are the values that are 1, 2, and 3 standard deviations below the mean, in that order? Write your answers separated by commas (for example: 1, 2, 3).\nWhich probability expression are we modelling? P(X&lt;3.2)P(X&gt;50)P(Z&gt;50)P(Z&lt;50)\nNow that we have identified the mean, standard deviation, and the type of relationship we want to model, we can start drawing our diagram. Try having a go below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "4  Hypothesis Testing",
    "section": "",
    "text": "4.1 Foundations of Hypothesis Testing\nNow that we’re familiar with variables and probability distributions, we can start exploring hypothesis testing. Hypothesis testing provides a structured framework for approaching statistical problems and drawing conclusions from data.\nWhen performing hypothesis testing, it’s helpful to reframe the problem in the context of who, what, and why.\nLet’s refresh our understanding of a few important concepts that will be helpful in reframing our problem:\nA target population is the broader group you are interested in studying. In other words, who do you want your results to apply to?\nBecause it is usually not practical to study the entire population, we collect a sample. It is a subset of observations that represents the population.\nA response variable is the variable of interest. It is what you want to measure or understand about the population. For example, if you are studying weight change after taking a new drug, the response variable is weight.\nUsually, you are interested in comparing something about this variable, such as a mean or proportion of a population. This is called a parameter of interest.\nBecause it is often not possible or feasible to collect the actual parameter of interest, such as a population mean, we can estimate it from a sample with a point estimate, such as a sample mean or sample proportion.\nAn explanatory variable is the variable that may influence or explain changes in the response variable. In the weight example, the explanatory variable would be whether or not someone took the drug. This helps us understand why the response variable changes, in this case, why weight might increase or decrease.\nTo ensure that any observed effect (such as weight gain) is actually due to the drug and not other factors, researchers often design experiments with a control group and a treatment group. The control group does not receive the treatment, while the treatment group does. The results are then compared between the two groups.\nThe null hypothesis, denoted as \\(H_0\\), represents the baseline assumption that there is no difference or effect. In the context of our drug example, this would mean that taking the drug does not influence weight.\nThe alternative hypothesis, denoted as \\(H_a\\)​, represents the belief that there is a real difference or effect. For the drug example, this would mean that taking the drug does have a significant influence on weight.\nTest your understanding with the exercise below.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapter3.html#foundations-of-hypothesis-testing",
    "href": "chapter3.html#foundations-of-hypothesis-testing",
    "title": "4  Hypothesis Testing",
    "section": "",
    "text": "Target population and sample\nExplanatory and response variables\nControl and treatment\nNull and Alternative hypotheses",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapter3.html#exercise-checkpoint-1",
    "href": "chapter3.html#exercise-checkpoint-1",
    "title": "4  Hypothesis Testing",
    "section": "4.2 Exercise Checkpoint 1",
    "text": "4.2 Exercise Checkpoint 1\n\nTest your understanding with the exercise below.\nUNE Sport is trialing a new get-fit training program for UNE students and wants to verify whether the program improves the fitness of the participants.\nOne hundred randomly chosen students are going to take 1 hour boot camp sessions 3 times a week for 4 weeks. At both the start and end of the month all students are going to be tested for the time in which they can run the 100m. The difference in this sprint time will be recorded.\nMatch the terms below with the correct options:\nNow, think about the hypotheses:\nWhat is the alternative hypothesis for the UNE Sport bootcamp study?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapter2.html#the-normal-distribution",
    "href": "chapter2.html#the-normal-distribution",
    "title": "3  Probability Distributions",
    "section": "",
    "text": "approximately 68% of data points within 1 standard deviation of the mean\napproximately 95% of data points within 2 standard deviations of the mean\napproximately 99.7% (i.e., almost all) data points within 3 standard deviation of the mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nWe are looking for \\(P(X&gt;50)\\), where \\(X \\sim (45, 3.2^2)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapter2.html#exercise-checkpoint-1",
    "href": "chapter2.html#exercise-checkpoint-1",
    "title": "3  Probability Distributions",
    "section": "3.2 Exercise Checkpoint 1",
    "text": "3.2 Exercise Checkpoint 1\n\nPractice drawing\na. P(Z &lt; −1.35)\nb. P(Z &gt; 1.48)\nc. P(−0.4 &lt; Z &lt; 1.5)\nd. P(|Z| &gt; 2)\ne.\n\nIn the previous chapter, we learnt about discrete and continuous numerical variables.\nYou may be familiar with the bell curve",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapter3.html#are-airlines-safer",
    "href": "chapter3.html#are-airlines-safer",
    "title": "4  Hypothesis Testing",
    "section": "6.1 Are airlines safer",
    "text": "6.1 Are airlines safer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "5  t-tests: one sample and two sample",
    "section": "",
    "text": "6 One Sample t-test\nWe will be covering a number of statistical tests throughout this course. Which statistical test you choose, depends on the type of experiment or study has been conducted and what type of data we have. In this chapter, we will specifically focus on applying one-sample and two-sample t-tests.\nInstead of using the normal (Z) distribution, t-tests use the t-distribution. This is because t-tests are usually based on smaller sample sizes, so we need to account for extra uncertainty in the standard error and critical values. As the sample size gets bigger, the t-distribution becomes almost identical to the Z-distribution, as can be seen below.\nLike the previous chapter, we can use a hypothesis testing framework to approach our research questions.\nBefore starting this lesson, you’ll need to download some data.\nLet’s apply our skills to some real data provided by the NSW Government, and see how even the skills we have learnt in three chapters can already be useful to the real world.\nData.NSW is an open data poral with over 16,724 NSW public sector datasets to download. You can also access additional Open Data Portals listed here:\nWe will look at the 2023 Early Childhood Education and Care (ECEC) survey. This survey collected responses from around 2,000 NSW parents and carers during January and February 2023, and was designed to understand which policy options families value most and what barriers affect their access to early childhood education and care. The dataset includes a broad mix of information: demographics, employment patterns, commuting times, childcare usage, costs, and satisfaction ratings. Because it contains both continuous and categorical variables, it provides a useful starting point for learning how to apply statistical tests to real-world data.\nWorking with real survey data can be messy, so we will go step by step and decide what needs cleaning and what does not. We’ll begin by loading the required packages and importing the dataset.\nlibrary(dplyr) \nlibrary(tidyr) \nlibrary(ggplot2)  \necec &lt;- read.csv(\"Data sets/ecec-survey-2023.csv\")\nLet’s take a look at the first six rows.\nstr(ecec)\n\n'data.frame':   2015 obs. of  678 variables:\n $ Row                 : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CaseID              : int  1 2 3 4 5 6 7 8 9 10 ...\n $ RecallCode          : chr  \"af0c9b6f-d7bb-4763-a47f-1684ec1b9024\" \"31c60a20-c8ef-4114-bcc8-59151000fff1\" \"8dba2e62-a33f-4974-8bdf-9ebfcdfc0216\" \"711984d0-d943-405e-9fab-267a4e8e4087\" ...\n $ Segment6            : chr  \"Non User Sydney\" \"Non User Sydney\" \"Full User Other NSW\" \"Full User Sydney\" ...\n $ SubmittedDate       : chr  \"24-Jan-23\" \"24-Jan-23\" \"24-Jan-23\" \"24-Jan-23\" ...\n $ ResponseTimeMinutes : num  33.37 15.59 11.81 14.05 7.35 ...\n $ BWS_BLOCK           : chr  \"Block 4\" \"Block 1\" \"Block 3\" \"Block 3\" ...\n $ DCE_BLOCK           : chr  \"Block 5\" \"Block 9\" \"Block 8\" \"Block 9\" ...\n $ SQ1                 : chr  \"Male\" \"Male\" \"Female\" \"Female\" ...\n $ SQ1_OTH             : logi  NA NA NA NA NA NA ...\n $ SQ2                 : int  1978 1999 1989 1977 1990 2000 2003 1978 1989 1978 ...\n $ SQ3                 : chr  \"Sydney\" \"Sydney\" \"Other NSW\" \"Sydney\" ...\n $ SQ4                 : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ SQ5                 : chr  \"Yes, I am the parent and have childcare responsibilities\" \"Yes, I am the parent and have childcare responsibilities\" \"Yes, I am the parent and have childcare responsibilities\" \"Yes, I am the parent and have childcare responsibilities\" ...\n $ SQ6_1               : int  1 1 0 0 1 1 1 1 0 0 ...\n $ SQ6_2               : int  0 0 0 0 1 0 0 0 0 1 ...\n $ SQ6_3               : int  0 0 1 1 0 0 0 0 1 0 ...\n $ SQ6_4               : int  1 0 0 1 0 0 0 0 0 0 ...\n $ SQ6_5               : int  0 0 0 0 0 0 0 2 0 0 ...\n $ SQ6_6               : int  0 0 0 1 0 0 1 2 0 0 ...\n $ GROUP               : chr  \"ECEC non-user household\" \"ECEC non-user household\" \"ECEC full user household\" \"ECEC full user household\" ...\n $ TXTREP1             : chr  \"Please fill in the table below for each child under the age of 18, from the youngest to the oldest.\" \"Please fill in the table below for your child under the age of 18.\" \"Please fill in the table below for your child under the age of 18.\" \"Please fill in the table below for each child under the age of 18, from the youngest to the oldest.\" ...\n $ A1_1                : chr  \"June\" \"January\" \"November\" \"January\" ...\n $ A1_2                : int  2010 2018 2018 2006 2021 2020 2019 2006 2021 2018 ...\n $ A1_3                : chr  \"Female\" \"Male\" \"Female\" \"Male\" ...\n $ A1_4                : chr  \"No\" \"No\" \"Yes, this Child has a Major Health Issue\" \"No\" ...\n $ A1_5                : chr  \"No\" \"No\" \"Yes, I receive financial support from the government for this child (other than the Child Care Subsidy)\" \"No\" ...\n $ A2_1                : chr  \"March\" \"\" \"\" \"March\" ...\n $ A2_2                : int  2019 NA NA 2010 2021 NA 2004 2007 NA NA ...\n $ A2_3                : chr  \"Female\" \"\" \"\" \"Male\" ...\n $ A2_4                : chr  \"No\" \"\" \"\" \"No\" ...\n $ A2_5                : chr  \"\" \"\" \"\" \"No\" ...\n $ A3_1                : chr  \"\" \"\" \"\" \"February\" ...\n $ A3_2                : int  NA NA NA 2019 NA NA NA 2012 NA NA ...\n $ A3_3                : chr  \"\" \"\" \"\" \"Female\" ...\n $ A3_4                : chr  \"\" \"\" \"\" \"No\" ...\n $ A3_5                : chr  \"\" \"\" \"\" \"No\" ...\n $ A4_1                : chr  \"\" \"\" \"\" \"\" ...\n $ A4_2                : int  NA NA NA NA NA NA NA 2014 NA NA ...\n $ A4_3                : chr  \"\" \"\" \"\" \"\" ...\n $ A4_4                : chr  \"\" \"\" \"\" \"\" ...\n $ A4_5                : chr  \"\" \"\" \"\" \"\" ...\n $ A5_1                : chr  \"\" \"\" \"\" \"\" ...\n $ A5_2                : int  NA NA NA NA NA NA NA 2021 NA NA ...\n $ A5_3                : chr  \"\" \"\" \"\" \"\" ...\n $ A5_4                : chr  \"\" \"\" \"\" \"\" ...\n $ A5_5                : chr  \"\" \"\" \"\" \"\" ...\n $ A6_1                : chr  \"\" \"\" \"\" \"\" ...\n $ A6_2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ A6_3                : chr  \"\" \"\" \"\" \"\" ...\n $ A6_4                : chr  \"\" \"\" \"\" \"\" ...\n $ A6_5                : chr  \"\" \"\" \"\" \"\" ...\n $ A7_1                : chr  \"\" \"\" \"\" \"\" ...\n $ A7_2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ A7_3                : chr  \"\" \"\" \"\" \"\" ...\n $ A7_4                : chr  \"\" \"\" \"\" \"\" ...\n $ A7_5                : chr  \"\" \"\" \"\" \"\" ...\n $ A8_1                : chr  \"\" \"\" \"\" \"\" ...\n $ A8_2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ A8_3                : chr  \"\" \"\" \"\" \"\" ...\n $ A8_4                : chr  \"\" \"\" \"\" \"\" ...\n $ A8_5                : chr  \"\" \"\" \"\" \"\" ...\n $ A9_1                : logi  NA NA NA NA NA NA ...\n $ A9_2                : logi  NA NA NA NA NA NA ...\n $ A9_3                : logi  NA NA NA NA NA NA ...\n $ A9_4                : logi  NA NA NA NA NA NA ...\n $ A9_5                : logi  NA NA NA NA NA NA ...\n $ Q1_1                : chr  \"YES\" \"YES\" \"\" \"\" ...\n $ Q1_2                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q1_3                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q1_4                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q1_5                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q1_6                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q1_7                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q1_8                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q1_OTH              : chr  \"\" \"\" \"\" \"\" ...\n $ Q1N                 : chr  \"1\" \"1\" \"\" \"\" ...\n $ Q2_1                : int  100 100 NA NA 100 100 35 100 NA NA ...\n $ Q2_2                : int  NA NA NA NA NA NA 65 NA NA NA ...\n $ Q2_3                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q2_4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q2_5                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q2_6                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q2_7                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q2_8                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q3_1                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q3_2                : chr  \"NO\" \"NO\" \"\" \"\" ...\n $ Q3_3                : chr  \"YES\" \"YES\" \"\" \"\" ...\n $ Q4                  : chr  \"\" \"\" \"\" \"\" ...\n $ Q5                  : chr  \"3\" \"4\" NA NA ...\n $ Q6_1                : int  NA NA NA NA 10 NA NA NA NA 8 ...\n $ Q6_2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q6_3                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q6_4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q6N                 : int  NA NA NA NA 10 NA NA NA NA 8 ...\n $ Q7_1                : chr  \"\" \"\" \"\" \"\" ...\n $ Q7_2                : chr  \"\" \"\" \"\" \"\" ...\n $ Q7_3                : chr  \"\" \"\" \"\" \"\" ...\n $ Q7_4                : chr  \"\" \"\" \"\" \"\" ...\n  [list output truncated]\nWow! There is a lot going on. Don’t worry about the NA values you see. Missing data is extremely common in large surveys, and we’ll make sure to remove any NA entries that would interfere with our analysis.\nYou may also notice that some of the variable names aren’t very descriptive. That can happen depending on how the original survey was designed or exported. Instead of renaming everything, we’ll concentrate on the specific variables we need for our t-test. If you want to see the exact wording of the survey questions, you can refer to the questionnaire provided alongside the dataset here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>t-tests: one sample and two sample</span>"
    ]
  },
  {
    "objectID": "chapter3.html#section-1",
    "href": "chapter3.html#section-1",
    "title": "4  Hypothesis Testing",
    "section": "6.1 ",
    "text": "6.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "6  Paired t-test and ANOVA",
    "section": "",
    "text": "7 Hypothesis tests using the t-distribution\n\n\n\n\n8 Paired t-test\n\nPhoto by Dan Gold on Unsplash\nThe next dataset we will be using comes from the Living Well Multicultural – Lifestyle Modification Program. This file contains paired pre- and post-intervention measurements for adults who participated in the program between 2014 and 2017.\nYou can then save the file in your Datasets folder in your InferentialStats project folder that you created in the Introduction. You can give it a new name so that it is easier to call into R Studio.\nIf you open the file, you may notice that the first row is simply a title. We can ignore the title by including skip when we load in the data.\nOnce you’ve saved the file, you can load the dataset into R\n\nType the code for loading in tidyverse. \nType the code to load in the data set. Name your dataset living_well. Be sure to convert strings to factors. \n\n\n\nI need a hint\n\n\nlibrary(ggplot2) \nlibrary(tidyverse)  \nliving_well &lt;- read.csv(\"Data sets/Cleaned_data_pre_post.csv\", \n                   stringsAsFactors = TRUE) \n\n\nWe can take a look at the data to ensure it has been loaded in correctly.\n\nWhat is the code for looking at the structure of the dataset?. \n\n\n\nShow code\n\n\nstr(living_well)\n\n'data.frame':   1112 obs. of  100 variables:\n $ Time           : Factor w/ 2 levels \"Baseline\",\"Post week8\": 1 1 1 1 1 1 1 1 2 2 ...\n $ Community      : Factor w/ 9 levels \"Afghani\",\"Arabic-speaking\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ Location       : Factor w/ 30 levels \"44\",\"Acacia Ridge\",..: 6 6 6 6 6 6 6 6 6 6 ...\n $ ID             : Factor w/ 554 levels \"ABA200879\",\"ABD 010160\",..: 295 294 38 296 191 192 297 10 295 294 ...\n $ Drop_yn        : Factor w/ 1 level \"Complete\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Dose_yn        : Factor w/ 2 levels \"&lt;=6 doses, Incomplete\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ Dose           : int  8 8 8 8 7 7 7 8 8 8 ...\n $ Age            : int  21 20 49 19 52 20 22 55 21 20 ...\n $ Gender         : Factor w/ 2 levels \"Female\",\"Male\": 2 1 1 1 2 2 1 2 2 1 ...\n $ Country_b      : Factor w/ 23 levels \"1 Afghanistan\",..: 22 22 22 22 22 17 22 22 22 22 ...\n $ Country_b_other: Factor w/ 21 levels \" \",\"Australia\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Time_aus       : Factor w/ 6 levels \"&gt;20 years\",\"1-5 years\",..: 2 4 3 4 3 3 3 4 2 4 ...\n $ language       : Factor w/ 18 levels \" \",\"1 Arabic\",..: 2 2 3 2 3 17 17 2 2 2 ...\n $ language_others: Factor w/ 51 levels \" \",\"20 Samoan\",..: 25 25 1 25 1 1 1 1 25 25 ...\n $ Ethnicity      : Factor w/ 82 levels \" \",\"Acholi\",\"Afghan\",..: 1 1 1 1 1 1 1 35 1 1 ...\n $ Postcode_SEIFA : Factor w/ 4 levels \"999\",\"High\",\"Low\",..: 4 4 2 4 2 2 2 4 4 4 ...\n $ Postcode       : int  4109 4109 4152 4109 4152 4152 4152 4109 4109 4109 ...\n $ Edu_combine    : Factor w/ 5 levels \"999\",\"Bachelor‎/Postgraduate\",..: 3 3 5 3 3 3 2 5 3 3 ...\n $ Education      : Factor w/ 8 levels \"999\",\"Bachelor degree\",..: 6 6 4 6 5 5 2 4 6 6 ...\n $ Country_Q      : Factor w/ 4 levels \" \",\"999\",\"Australia\",..: 3 3 4 3 4 3 3 4 3 3 ...\n $ Employ_comb    : Factor w/ 5 levels \"999\",\"No paid work\",..: 4 4 3 4 3 4 5 5 4 4 ...\n $ Employment     : Factor w/ 11 levels \"999\",\"Casual paid work\",..: 9 9 7 9 6 9 2 2 9 9 ...\n $ Household      : Factor w/ 6 levels \"999\",\"Couple, no children\",..: 6 6 3 6 3 6 6 3 6 6 ...\n $ Referral       : Factor w/ 13 levels \" \",\"1 ECCQ website/newsletter\",..: 1 9 9 9 9 3 3 9 1 9 ...\n $ Weight         : num  55.5 48.5 88.9 52.3 90.7 75.7 54 77 55 48 ...\n $ Height         : num  167 161 160 159 173 ...\n $ BMI_cate2      : Factor w/ 5 levels \"999\",\"Normal weight\",..: 2 2 3 2 3 2 2 4 2 2 ...\n $ BMI_cate1      : Factor w/ 5 levels \"999\",\"Normal weight\",..: 2 2 3 2 3 2 2 4 2 2 ...\n $ BMI            : num  19.9 18.8 34.6 20.8 30.3 24.5 21.5 26.2 19.7 18.6 ...\n $ Waist          : num  74 71 106 70 102 ...\n $ Risk_new       : Factor w/ 3 levels \"999\",\"No\",\"Yes\": 2 2 3 2 3 2 2 2 2 2 ...\n $ Risk           : Factor w/ 4 levels \" \",\"999\",\"No\",..: 3 3 4 3 4 3 3 3 3 3 ...\n $ WHtR_yn        : Factor w/ 3 levels \"999\",\"High risk\",..: 3 3 2 3 2 2 3 2 3 3 ...\n $ WHtR           : num  0.44 0.44 0.66 0.44 0.59 0.53 0.46 0.56 0.44 0.44 ...\n $ BP_systolic    : int  110 88 111 110 140 109 110 146 90 102 ...\n $ BP_diastolic   : int  70 54 63 60 58 59 64 78 75 60 ...\n $ High_BP        : Factor w/ 4 levels \" \",\"999\",\"No\",..: 3 3 3 3 3 3 3 4 3 3 ...\n $ Weight_pc_cate : Factor w/ 6 levels \"999\",\"A healthy weight\",..: 2 3 3 6 3 3 4 3 2 2 ...\n $ Weight_pc      : Factor w/ 7 levels \" \",\"1 A healthy weight\",..: 2 3 3 5 3 3 6 3 2 2 ...\n $ Weight_stt     : Factor w/ 6 levels \" \",\"1 Lost weight\",..: 5 3 4 3 2 3 4 3 4 2 ...\n $ Chronic_con    : Factor w/ 4 levels \" \",\"999\",\"No\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ Chronic_name   : Factor w/ 174 levels \" \",\"Arteritis\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Smoke_stt      : Factor w/ 4 levels \" \",\"999\",\"No\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ Smoke_re       : Factor w/ 8 levels \" \",\"3\",\"999\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ Smoke_num      : Factor w/ 24 levels \" \",\"0\",\"0 (quit)\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Alcohol_stt    : Factor w/ 4 levels \" \",\"999\",\"No\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ Alcohol_safe   : Factor w/ 4 levels \" \",\"999\",\"Not safe\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ Alcohol_re     : Factor w/ 5 levels \" \",\"999\",\"I don't consume alcohol\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ Alcohol_freq   : Factor w/ 9 levels \" \",\"1 Everyday\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Alcohol_amt    : Factor w/ 6 levels \" \",\"1 Less than 1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Fruit_yn       : Factor w/ 3 levels \"999\",\"Meet the guideline\",..: 2 2 2 2 2 3 2 2 2 2 ...\n $ Fruit_cate     : Factor w/ 8 levels \" \",\"999\",\"Don't eat fruit\",..: 4 8 7 4 4 6 4 4 7 7 ...\n $ Fruit          : Factor w/ 9 levels \" \",\"0 Don't eat fruit\",..: 8 6 7 8 8 5 8 8 7 7 ...\n $ Vegetables_yn  : Factor w/ 3 levels \"999\",\"Don't meet the guideline\",..: 2 2 2 2 2 2 2 2 2 3 ...\n $ Veggie_cate    : Factor w/ 9 levels \" \",\"999\",\"Don't eat vegetables\",..: 7 8 6 8 8 3 8 7 9 4 ...\n $ Veggie         : Factor w/ 10 levels \" \",\"0 Don't eat vegetables\",..: 5 7 4 7 7 2 7 5 6 9 ...\n $ Milk_combine   : Factor w/ 5 levels \" \",\"999\",\"Full fat‎/cream\",..: 3 5 4 3 3 3 3 3 4 4 ...\n $ Milk_cate      : Factor w/ 10 levels \" \",\"999\",\"Don't consume any milk\",..: 10 3 6 10 10 10 10 10 6 6 ...\n $ Milk           : Factor w/ 14 levels \" \",\"0 Don't consume any milk\",..: 5 4 6 5 5 5 5 5 6 6 ...\n $ Takeaway_comb  : Factor w/ 5 levels \"&lt;= 1 a week\",..: 3 1 1 3 1 5 1 1 1 1 ...\n $ Takeaway_cate  : Factor w/ 9 levels \" \",\"2-3 times per week\",..: 2 7 7 2 7 8 7 7 9 7 ...\n $ Takeaway       : Factor w/ 10 levels \" \",\"0 Never or rarely\",..: 5 3 3 5 3 2 3 3 4 3 ...\n $ Chips_comb     : Factor w/ 6 levels \" \",\"&lt;=1 per week\",..: 4 2 2 2 2 6 2 2 2 2 ...\n $ Chips_cate     : Factor w/ 9 levels \" \",\"2-3 times per week\",..: 2 9 7 9 7 8 7 7 7 7 ...\n $ Chips          : Factor w/ 12 levels \" \",\"0 Never or rarely\",..: 6 5 4 5 4 2 4 4 4 4 ...\n $ Salty_snk3gr   : Factor w/ 5 levels \" \",\"1-3 times‎ per week\",..: 2 2 5 2 5 2 5 5 5 5 ...\n $ Salty_snk_new  : Factor w/ 6 levels \" \",\"&gt;=4 times‎/week\",..: 3 3 5 3 6 3 6 5 6 6 ...\n $ Salty_snk_cate : Factor w/ 8 levels \" \",\"1-3 times week\",..: 2 2 6 2 7 2 7 6 7 7 ...\n $ Salty_snk      : Factor w/ 8 levels \" \",\"0 Never or rarely\",..: 4 4 3 4 2 4 2 3 2 2 ...\n $ Sweet_snk3gr   : Factor w/ 5 levels \" \",\"1-3 times per week\",..: 3 5 2 3 5 2 2 3 5 5 ...\n $ Sweet_snk_cate : Factor w/ 8 levels \" \",\"2 or more times per day\",..: 5 7 4 2 6 4 4 8 6 6 ...\n $ Sweet_snk      : Factor w/ 9 levels \" \",\"0 Don’t consume any milk\",..: 6 3 5 8 4 5 5 7 4 4 ...\n $ Softdrink_3gr  : Factor w/ 4 levels \"1-3 times per week\",..: 2 4 1 4 1 2 4 1 1 4 ...\n $ Softdrink_cate : Factor w/ 8 levels \" \",\"2 or more times per day\",..: 2 7 4 7 4 8 6 4 4 6 ...\n $ Softdrink      : Factor w/ 8 levels \" \",\"0 Never or rarely\",..: 7 2 4 2 4 6 3 4 4 3 ...\n $ Pr_meat_3gr    : Factor w/ 4 levels \"1-2 times per week\",..: 2 1 4 2 1 2 4 4 4 4 ...\n $ Pr_meat        : Factor w/ 7 levels \" \",\"1-2 times per week\",..: 4 2 6 4 2 3 6 6 6 6 ...\n $ Processed_meat : Factor w/ 7 levels \" \",\"0 Never or rarely\",..: 6 4 3 6 4 5 3 3 3 3 ...\n $ Vegetarian     : Factor w/ 11 levels \" \",\"30\",\"300\",..: 10 10 10 10 10 10 10 10 10 10 ...\n $ Walking        : int  90 95 0 120 45 100 200 120 120 250 ...\n $ Vigorous_lb    : int  60 100 30 30 150 10 200 900 60 70 ...\n $ Md_act         : int  60 30 5 10 0 0 30 90 60 30 ...\n $ Vigorous_act   : int  30 30 0 15 0 0 100 90 120 250 ...\n $ Act_comp_cate  : Factor w/ 7 levels \"999\",\"A little less\",..: 3 6 3 6 5 6 4 4 3 7 ...\n $ Act_comparison : Factor w/ 8 levels \" \",\"1 Significantly less\",..: 5 2 5 2 7 2 4 4 5 6 ...\n $ KL1            : Factor w/ 8 levels \" \",\"999\",\"A\",..: 7 6 6 4 8 5 6 6 6 6 ...\n $ KL2            : Factor w/ 8 levels \" \",\"999\",\"A\",..: 8 5 8 5 8 3 3 6 3 3 ...\n $ KL3            : Factor w/ 8 levels \" \",\"999\",\"A\",..: 6 5 5 5 8 7 7 5 5 5 ...\n $ KL4            : Factor w/ 7 levels \" \",\"999\",\"A\",..: 4 6 5 5 6 6 6 6 6 6 ...\n $ KL5            : Factor w/ 7 levels \" \",\"999\",\"A\",..: 7 7 3 4 7 5 7 5 5 5 ...\n $ KL6            : Factor w/ 7 levels \" \",\"999\",\"A\",..: 7 4 7 5 7 7 6 5 6 6 ...\n $ KL7            : Factor w/ 7 levels \" \",\"999\",\"A\",..: 5 5 6 6 5 5 5 5 5 5 ...\n $ conf_risk      : int  10 8 8 10 1 7 7 9 9 10 ...\n $ conf_manage    : int  10 8 8 10 1 6 7 10 9 10 ...\n $ Physic_act_yn  : Factor w/ 3 levels \"999\",\"No\",\"Yes\": 3 3 2 3 3 2 3 3 3 3 ...\n $ Physic_moderate: int  150 125 5 130 45 100 230 210 180 280 ...\n $ Physic_vigo    : int  90 130 30 45 150 10 300 990 180 320 ...\n $ Physic_all     : int  240 255 35 175 195 110 530 1200 360 600 ...\n $ Knowledge      : int  1 4 2 1 2 3 4 5 6 6 ...\n  [list output truncated]\n\n\n\n\nHow many variables are there? \nHow many observations are there? \n\n\nunique(living_well$Time)\n\n[1] Baseline   Post week8\nLevels: Baseline Post week8\n\n\nIf we look at the Time variable, we can see that there are two levels: Baseline and Post week8. We also have an ID variable, which indicates that we have a repeated measurement for the baseline and post week 8 measurements. As we have one row for each measurement, this means our data is in long format. Another way to think about long format is that the same participant appears multiple times in the data set.\nTo be able to perform a paired t-test to compare the pre- and post-measurements, we need to convert the data to wide format. This means that we want one row per participant, with separate columns for each time point, such as a pre value and post value in the same row.\nYou may notice that there are many rows that have observations recorded as 999. At times, 999 is used as a placeholder for missing entries.\n\nliving_well_wide &lt;- pivot_wider(\n  living_well,\n  id_cols = ID,               \n  names_from = Time,          \n  values_from = BMI           \n)\n\nWarning: Values from `BMI` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(ID, Time)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nYou can see an error appears when we try to convert the data. This is telling us there are duplicates for the ID beyond the two that we are expecting. Let’s try using the code provided in the warning to see what’s going on.\n\nliving_well |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(ID, Time)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n          ID       Time n\n1  PIR201181   Baseline 2\n2  PIR201181 Post week8 2\n3 ABD 010187   Baseline 2\n4 ABD 010187 Post week8 2\n\n\nThere are four IDs that have two baseline measurements and two post week8 measurements. Let’s investigate these rows closer.\n\nliving_well[living_well$ID == \"PIR201181\", c(\"ID\",\n                                             \"Time\", \n                                             \"BMI\", \n                                             \"Weight\") ]\n\n           ID       Time  BMI Weight\n647 PIR201181   Baseline 27.5   74.3\n651 PIR201181   Baseline 21.0   50.4\n655 PIR201181 Post week8 26.2   71.0\n659 PIR201181 Post week8 21.0   49.7\n\n\n\nliving_well[living_well$ID == \"ABD 010187\", c(\"ID\",\n                                             \"Time\", \n                                             \"BMI\", \n                                             \"Weight\") ]\n\n            ID       Time  BMI Weight\n813 ABD 010187   Baseline 26.4   63.4\n816 ABD 010187   Baseline 21.7   59.0\n831 ABD 010187 Post week8 26.7   64.2\n834 ABD 010187 Post week8 26.7   64.2\n\n\nThis confirms what we expected - that there are four measurements per ID, when we were expecting two. This was likely a data entry error, which can happen in real world datasets. It is impossible to tell which pre- and post-measurements match each other, so the safest option is to drop these observations. As we have 1112 observations, this shouldn’t impact our analysis too much.\n\nbad_ID &lt;- living_well |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(ID, Time)) |&gt;\n  dplyr::filter(n &gt; 1L) %&gt;%\n  distinct(ID)\n\nliving_well_clean &lt;- living_well %&gt;%\n  filter(!ID %in% bad_ID$ID)\n\nLet’s try convert our data to wide format again.\n\nliving_well_wide &lt;- pivot_wider(\n  living_well_clean,\n  id_cols = ID,               \n  names_from = Time,          \n  values_from = BMI,\n  names_glue = \"{str_replace_all(Time, ' ', '_')}\"\n)\n\nNo warning this time! That’s a good sign our method worked. Let’s take a look at our dataset now.\n\nstr(living_well_wide)\n\ntibble [552 × 3] (S3: tbl_df/tbl/data.frame)\n $ ID        : Factor w/ 554 levels \"ABA200879\",\"ABD 010160\",..: 295 294 38 296 191 192 297 10 453 452 ...\n $ Baseline  : num [1:552] 19.9 18.8 34.6 20.8 30.3 24.5 21.5 26.2 19.6 19.9 ...\n $ Post_week8: num [1:552] 19.7 18.6 33.7 19.6 30.2 24.3 20.7 25.7 19.5 19.8 ...\n\n\n\nHow many variables are there? \nHow many observations are there? \n\nRemember, we have 552 observations now because we combined the pre- and post-measurements into one row. We started with 1112, removed 8, and then combined the two measurements which results in 552.\nNow we can check if there are any 999 values and drop them.\n\nany(living_well_wide$Baseline == 999, na.rm = TRUE)\n\n[1] TRUE\n\nany(living_well_wide$Post_week8== 999, na.rm = TRUE)\n\n[1] TRUE\n\n\nAs we have TRUE for both variables, we can convert the 999 to NA and then remove.\n\nliving_well_wide &lt;- living_well_wide %&gt;%\n  mutate(\n    Baseline = na_if(Baseline, 999),\n    Post_week8 = na_if(Post_week8, 999)\n  ) %&gt;%\n  drop_na(Baseline, Post_week8)\n\nany(living_well_wide$Baseline == 999, na.rm = TRUE)\n\n[1] FALSE\n\nany(living_well_wide$Post_week8== 999, na.rm = TRUE)\n\n[1] FALSE\n\n\nWe have officially cleaned our data! We can now get into our statistical analysis.\n\n8.0.1 Write Hypotheses\nBecause we are keeping the dataset simple, a natural research question is whether there is a significant difference\n\\(H_0:\\) For participants in the Living Well Multicultural – Lifestyle Modification Program, the mean BMI after the program is not significantly different than the mean BMI before the program.\n\\(H_A:\\) For participants in the Living Well Multicultural – Lifestyle Modification Program, the mean BMI after the program is significantly lower than the mean BMI before the program.\nWe can express this in mathematical notation:\n\\(H_0:\\) \\(\\mu_{diff}=0\\)\n\\(H_A:\\) \\(\\mu_{diff} &gt; 0\\)\nwhere \\(diff = BMI_{Baseline} - BMI_{Post Week 8}\\). Now we need to calculate the difference between the baseline BMI and Post Week 8 BMI . We can do that with:\n\nliving_well_wide$diff &lt;- (living_well_wide$Baseline - living_well_wide$Post_week8)\n\nliving_well_wide$diff\n\n  [1]  0.20  0.20  0.90  1.20  0.10  0.20  0.80  0.50  0.10  0.10  0.00  0.40\n [13]  1.50  0.30  0.30 -0.10  0.10  0.80  1.10  1.20  0.80  0.30 -0.06  0.26\n [25]  0.45  0.53  0.32  0.42 -0.01 -0.03  0.31  0.60  0.50  0.70  0.40  0.40\n [37]  1.50  0.40  0.40  0.30  0.40  0.40  0.30  0.70  0.00  1.60  1.00 -0.20\n [49] -0.60  0.40  0.20  0.10  0.40  1.00  0.50  1.20  0.50  0.30  0.00 -0.10\n [61]  0.70 -1.70 -4.90  0.40  0.10  0.20  0.30  0.00  0.70  0.90  0.10  0.90\n [73]  0.60  0.10  1.70  0.40  0.10  0.30  1.10  0.20  0.10  0.20  0.00  0.40\n [85]  0.10 -0.40  0.00  0.40  0.20  0.00  0.30 -0.10  0.00  0.80  1.00  0.80\n [97]  0.40 -0.20  1.50  0.30  0.30 -0.20  0.70  0.40 -0.30  1.40  0.70  0.60\n[109]  0.50  0.30  0.30  1.80 -0.10  1.00  0.50  0.40 -0.20  0.90  0.50  0.50\n[121]  1.00  0.50  0.80  0.30  1.30  0.60  1.10  0.70  0.70  0.60  0.90  0.80\n[133]  0.80 -0.80  0.30  0.20  1.40  2.30 -0.70  0.70  1.10  0.40 -0.90  1.20\n[145] -1.70  2.00  0.00  1.40  1.40  3.60  0.80  0.40  0.80  1.00  0.40  0.50\n[157] -0.80  0.80  0.70  1.40  1.30  1.00  0.70 -0.20  0.70 -1.50 -2.10  2.00\n[169] -1.10 -0.60  0.80  1.00  0.10 -0.40  2.10 -0.30  0.90 -0.30  0.70 -0.30\n[181] -2.90 -0.50 -0.80  1.60  0.50  1.30  0.60  0.30  0.00  0.00  0.50  0.10\n[193]  0.60  0.00  0.90  0.30  0.90  0.00  0.00 -0.40 -0.40 -1.40  0.30  0.10\n[205] -1.60  0.00  0.20  0.50  0.20  0.20  0.00  0.00  0.50  0.00  0.20  0.40\n[217] -0.30  0.30  0.00  0.10  0.40 -0.50  0.30  0.40 -0.90  0.00 -0.20 -0.50\n[229]  0.80  0.30  0.60  0.10  0.10 -0.40  0.20  0.20  0.10  0.30  0.10  0.20\n[241]  0.20  0.00  0.10  0.40  0.40  0.00  0.00  0.10  0.00  0.20  0.90  0.90\n[253]  0.30  0.70  0.40  0.70  0.70  0.00  0.40  1.00  0.90  0.30  0.00  1.20\n[265] -0.40  0.30  1.40  0.50 -0.30 -0.40 -0.10 -0.30  0.30 -0.40 -0.20  0.70\n[277]  0.40  0.10  1.30  0.00  0.80 -0.70  0.40  0.30  0.00  0.40  0.50  0.00\n[289] -2.00  0.40  0.40  0.40  0.60  0.60 -0.10  0.30  0.50  0.60 -0.30  0.30\n[301] -0.30 -0.30  0.20  1.00  0.30  1.00  0.80 -0.20 -0.80  0.00 -1.00 -0.40\n[313]  0.00  0.90 -0.50  0.20 -0.90 -0.30  0.60  0.10  0.40  0.10  0.30  0.50\n[325]  1.20 -0.10  0.20  0.10  0.70  0.20  0.00  0.80  0.90 -0.90  0.50  0.20\n[337]  0.30  0.50  0.80  1.40  0.60 -0.40  0.30  0.80  0.80 -0.20  0.10  0.20\n[349] -0.10  0.80  0.50 -0.10  1.00  0.10  0.20  0.60  0.10 -0.80  0.30  0.20\n[361]  0.60  0.70  0.80  0.70  0.20  0.90  0.60  0.60  0.40  1.10  1.60 -0.20\n[373] -0.30  0.70  0.10 -0.60  0.30  0.30  0.30  0.30 -0.10  0.50  0.80 -0.30\n[385]  0.70 -0.30  0.90 -0.90  0.10  1.50  1.60  0.90 -0.20 -0.30  0.70  1.20\n[397]  1.90  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 -0.10 -0.70  0.00\n[409]  0.00  0.00 -0.10  0.70 -0.50  0.80  0.20  0.30  0.20  0.20  0.60  0.00\n[421]  0.40  0.60  0.20  0.00  0.40  0.40  0.40 -0.50 -0.60 -0.50  1.10 -1.00\n[433]  0.10  0.20  0.70  0.30 -1.10  0.60 -0.30 -0.80 -0.10  0.00 -0.30  0.30\n[445]  0.70  0.10  0.20 -1.30 -0.30 -0.30 -0.10  1.20  0.60  0.30 -0.30  1.60\n[457]  1.20  0.20  0.00  0.70  0.20  0.00  0.00  0.20 -0.10  0.40  0.30  0.60\n[469]  0.30  1.20  1.40  0.70  0.20  0.10  0.00  0.20 -0.20 -0.50 -0.10  0.20\n[481] -0.30 -0.30 -2.10  0.70 -0.90 -1.30  1.00  0.30  0.00  0.00 -0.70  0.90\n[493] -0.90  0.20 -0.20 -0.40  0.40 -0.20 -0.40 -0.70  0.80  1.10 -0.40  1.80\n[505]  0.30 -0.30  0.70  0.40  2.20  0.20  0.60  0.10  3.70  1.10  0.00  1.30\n[517]  0.80  0.50  0.40  0.80  0.10  0.40 -0.10  0.10  0.70 -1.20  0.40  0.50\n[529]  0.10  0.20  0.10  0.20  0.00  0.00  0.00  0.50  0.30  0.00  0.80 -0.20\n[541]  0.00  0.80 -0.10  0.40  0.10 -3.80  0.90  0.20  0.00\n\n\nWe can attach the data set.\n\nattach(living_well_wide)\n\nWe can check for skewness and normality.\n\nggplot(data=living_well_wide, aes(y=diff, x=\"\"))+\n  geom_boxplot(fill=\"mistyrose\", col=\"pink4\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data=living_well_wide, aes(sample=diff))+\n  geom_qq(col=\"navyblue\")+\n  geom_qq_line(col=\"steelblue\")+\n  labs(x=\"Theoretical quantiles\", y=\"Sample quantiles\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can now run our t-test:\n\nt.test(Baseline, Post_week8, paired = TRUE, alternative = \"greater\") \n\n\n    Paired t-test\n\ndata:  Baseline and Post_week8\nt = 9.0896, df = 548, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 0.2275603       Inf\nsample estimates:\nmean difference \n      0.2779417 \n\n\nThe p-value (\\(t_{548}=9.09, p=2.2*10^{-16}\\)) is much less than the 5% significance level (p&lt;0.05). We used greater because our hypothesis was that BMI would go down after the program. The t test calculates the difference as baseline minus post week 8, so if BMI really decreased, that difference would be positive. Using greater than tells R to test whether the average of those differences is greater than zero, meaning baseline values were higher than post week 8 values.\nTherefore, the mean BMI after the program is significantly lower than the mean BMI before the program.\nLet’s generate a two-sided paired t-test to get the confidence interval.\n\nt.test(Baseline, Post_week8, paired = TRUE) \n\n\n    Paired t-test\n\ndata:  Baseline and Post_week8\nt = 9.0896, df = 548, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.2178774 0.3380060\nsample estimates:\nmean difference \n      0.2779417 \n\n\nWith 95% confidence, there ia mean BMI difference of 0.22 and 0.34 BMI Because this interval includes zero, the data supports a real difference before and after the program. Note that if 0 was IN in the confidence interval, this would suggest there is not a significant difference.\n\ndetach(living_well_wide)\n\n\n\n\n9 ANOVA\nFor the second part of this lesson, we are going to explore ANOVA.\nWe want to do ANOVA when we are comparing three or more groups (or categories) against a continuous response variable.\nWe can continue on with our living_well dataset, but instead refocus our research question and restructure the dataset accordingly.\nAn example of a research question we might ask with ANOVA is whether education level impacts post program BMI.\nLet’s rewrite our hypotheses.\n\\(H_0:\\) There is no difference in mean BMI at post week 8 between education groups in the Living Well Multicultural – Lifestyle Modification Program.\n\\(H_A:\\) At least one education group has a different mean BMI at post week 8.\nWe can express this in mathematical notation:\n\\(H_0:\\) \\(\\mu_1=\\mu_2=\\mu_3=...=\\mu_7\\)\n\\(H_A:\\) At least one \\(\\mu_i\\) is different.\nLet’s tidy up the data.\n\nliving_well_aov &lt;- living_well_clean %&gt;%\n  pivot_wider(\n    id_cols = c(ID, Education),\n    names_from = Time,\n    values_from = BMI,\n    names_glue = \"{str_replace_all(Time, ' ', '_')}\"\n  )\n\nLet’s be sure to remove any 999 values. How can we do this?\n\n\nI need a hint\n\n\nliving_well_aov &lt;- living_well_aov %&gt;%\n  mutate(Education = factor(replace(Education, Education == \"999\", NA))) %&gt;%\n  mutate(\n    Baseline = na_if(Baseline, 999),\n    Post_week8 = na_if(Post_week8, 999)\n  ) %&gt;%\n  drop_na(Baseline, Post_week8, Education)\n\nRemember, because Education is a categorical variable, 999 will be stored as a string rather than a numerical variable.\n\nstr(living_well_aov)\n\ntibble [521 × 4] (S3: tbl_df/tbl/data.frame)\n $ ID        : Factor w/ 554 levels \"ABA200879\",\"ABD 010160\",..: 295 294 38 296 191 192 297 10 453 452 ...\n $ Education : Factor w/ 7 levels \"Bachelor degree\",..: 5 5 3 5 4 4 1 3 1 1 ...\n $ Baseline  : num [1:521] 19.9 18.8 34.6 20.8 30.3 24.5 21.5 26.2 19.6 19.9 ...\n $ Post_week8: num [1:521] 19.7 18.6 33.7 19.6 30.2 24.3 20.7 25.7 19.5 19.8 ...\n\nattach(living_well_aov)\n\n\nFor a one-way ANOVA we need to check\n\nIndependence\nNormality of the response variable for each population.\nAll populations have the same standard deviation.\n\nTo determine these:\n\nWe typically assume independence, as long as e.g., a random sample has been collected and each observation is a unique case/subject/participant.\nWe can determine normality by considering side-by-side boxplots, or a quantile-quantile plot.\nWe can compute the standard deviation for each group using aggregate() like we tried last week. A rule of thumb is that we are likely safe if the largest standard deviation is less than double the smallest standard deviation.\n\n\nggplot(living_well_aov, aes(x=Education, y=Post_week8))+\n  geom_boxplot(fill = c(\"#f94144\",\n                        \"#f3722c\",\n                        \"#f8961e\",\n                        \"#f9c74f\",\n                        \"#90be6d\",\n                        \"#43aa8b\",\n                        \"#577590\"))+\n  theme_bw() +   \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\n\nggplot(data=living_well_aov, aes(sample=Post_week8))+\n  geom_qq()+\n  geom_qq_line()+\n  theme_bw()+\n  facet_wrap(~Education)\n\n\n\n\n\n\n\n\n\naggregate(Post_week8~Education, FUN=sd)\n\n                        Education Post_week8\n1                 Bachelor degree   6.393231\n2 Certificate (trade or business)   8.088750\n3     Diploma or Associate degree   6.313799\n4     High school (up to 4 years)   8.743657\n5     High school (up to 6 years)   6.760607\n6              Posgraduate degree   4.549479\n7                  Primary school   7.602748\n\n\n\naggregate(Post_week8~Education, FUN=length)\n\n                        Education Post_week8\n1                 Bachelor degree         93\n2 Certificate (trade or business)         54\n3     Diploma or Associate degree         60\n4     High school (up to 4 years)         69\n5     High school (up to 6 years)         95\n6              Posgraduate degree         48\n7                  Primary school        102\n\n\nBased on these outputs, do you think the necessary conditions for our one-way ANOVA are met?\n\n\nI need a hint\n\nEach individual is unique and only sampled randomly once, so independence is met.\nThe scatter in the q-q plot is reasonably straight and the boxplots are reasonably symmetrical about the median, so a normal distribution is met.\n\n4.55/8.74\n\n[1] 0.520595\n\n\n\\(4.55/8.74 = 0.52 &lt; 2\\)\nThe largest standard deviation is less than twice that of the smallest, so constant variance is met.\n\nWe can now run an ANOVA on the data. Note these are two lines of code and thus need to be run one after the other.\n\nmod.aov &lt;- lm(Post_week8~Education, data = living_well_aov)\nanova(mod.aov)\n\nAnalysis of Variance Table\n\nResponse: Post_week8\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nEducation   6   771.1 128.516  2.5519 0.01915 *\nResiduals 514 25885.8  50.362                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe key values we can see are the F statistic, and the p-value that is associated with that. Given that our p-value is less than 0.05, we have evidence to reject the null hypothesis.\nIn regards to our research question we can say that:\nThere is a significant difference in mean BMI for at least some education levels.\nWe need to carefully check for differences between each group (correcting for multiple comparisons) to determine what is happening. To do this we can use a pairwise t-test with a Bonferroni correction. We use the Bonferroni adjustment because multiple pairwise comparisons increase the chance of finding a significant result just by chance. The adjustment makes the significance threshold more conservative, helping to control the overall Type I error rate (i.e. false positives) so that any detected differences are more likely to be real.\n\npairwise.t.test(Post_week8,Education,p.adj='bonf')\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Post_week8 and Education \n\n                                Bachelor degree Certificate (trade or business)\nCertificate (trade or business) 0.254           -                              \nDiploma or Associate degree     1.000           1.000                          \nHigh school (up to 4 years)     0.581           1.000                          \nHigh school (up to 6 years)     1.000           0.449                          \nPosgraduate degree              1.000           0.066                          \nPrimary school                  1.000           1.000                          \n                                Diploma or Associate degree\nCertificate (trade or business) -                          \nDiploma or Associate degree     -                          \nHigh school (up to 4 years)     1.000                      \nHigh school (up to 6 years)     1.000                      \nPosgraduate degree              0.922                      \nPrimary school                  1.000                      \n                                High school (up to 4 years)\nCertificate (trade or business) -                          \nDiploma or Associate degree     -                          \nHigh school (up to 4 years)     -                          \nHigh school (up to 6 years)     1.000                      \nPosgraduate degree              0.147                      \nPrimary school                  1.000                      \n                                High school (up to 6 years) Posgraduate degree\nCertificate (trade or business) -                           -                 \nDiploma or Associate degree     -                           -                 \nHigh school (up to 4 years)     -                           -                 \nHigh school (up to 6 years)     -                           -                 \nPosgraduate degree              1.000                       -                 \nPrimary school                  1.000                       1.000             \n\nP value adjustment method: bonferroni \n\n\nSince there are seven education groups, the pairwise t-tests compare the mean post-week-8 BMI between each pair of education levels. This means we can look at differences such as Bachelor degree vs Certificate (trade or business), Bachelor degree vs Diploma or Associate degree, or High school (up to 4 years) vs Postgraduate degree, and so on. The table shows the p-values for each of these comparisons, adjusted using the Bonferroni method.\nThe ANOVA showed a significant overall difference in post week 8 BMI between education groups (p = 0.019), meaning that at least one group had a different mean BMI compared to the others. However, when pairwise t tests were run with a Bonferroni adjustment, none of the individual group comparisons were statistically significant. This suggests that while there may be small differences in BMI across education levels overall, these differences are not strong enough between specific groups to remain significant once the stricter adjustment for multiple comparisons is applied.\nFinally, lets calculate some confidence intervals for each group. If two groups have confidence intervals that do not overlap, it suggests their mean BMI values are likely different. If the intervals overlap, we can’t say there is a clear difference between those groups.\n\n#Calculate CIs for each group\nmod.aov.ci &lt;- lm(Post_week8~Education-1, data = living_well_aov)\nci&lt;-confint(mod.aov.ci)\nci\n\n                                            2.5 %   97.5 %\nEducationBachelor degree                 26.50376 29.39517\nEducationCertificate (trade or business) 29.11016 32.90466\nEducationDiploma or Associate degree     27.80344 31.40322\nEducationHigh school (up to 4 years)     28.76072 32.11754\nEducationHigh school (up to 6 years)     26.78538 29.64620\nEducationPosgraduate degree              24.81475 28.83942\nEducationPrimary school                  27.75092 30.51182\n\n\n\n#Calculate means for each group\nmean_lw&lt;-aggregate(Post_week8 ~ Education, data = living_well_aov, FUN = mean)\n#Add the means and CIs together in a dataframe to plot\nvalues&lt;-cbind(mean_lw,ci)\n#Rename the column headings and row headings\ncolnames(values)&lt;-c(\"group\",\"mean\", \"lower\",\"upper\")\nrownames(values)&lt;-c(1, 2, 3, 4, 5, 6, 7)\nvalues\n\n                            group     mean    lower    upper\n1                 Bachelor degree 27.94946 26.50376 29.39517\n2 Certificate (trade or business) 31.00741 29.11016 32.90466\n3     Diploma or Associate degree 29.60333 27.80344 31.40322\n4     High school (up to 4 years) 30.43913 28.76072 32.11754\n5     High school (up to 6 years) 28.21579 26.78538 29.64620\n6              Posgraduate degree 26.82708 24.81475 28.83942\n7                  Primary school 29.13137 27.75092 30.51182\n\n\n\n#create errorbar plot\nggplot(data=values, aes(x=group, y=mean))+\n  geom_errorbar(aes(ymin=lower,\n                    ymax=upper), \n                linewidth=1.5, \n                col=c(\"#f94144\",\n                      \"#f3722c\",\n                      \"#f8961e\",\n                      \"#f9c74f\",\n                      \"#90be6d\",\n                      \"#43aa8b\",\n                      \"#577590\"))+\n  geom_point(size=2, \n             col=\"#0F151A\")+\n  theme_bw()+\n  labs(y=\"BMI (mean+CIs)\") +\n  theme(axis.text.x = element_text(angle = 90, \n                                   vjust = 0.5, \n                                   hjust = 1))\n\n\n\n\n\n\n\n\nBecause these intervals overlap, it suggests that the mean post-week-8 BMI values are quite similar across education groups. There isn’t clear separation between any of the groups, which means we can’t say any one education group’s mean BMI is significantly different from the others based on these confidence intervals.\nTherefore, the ANOVA indicated a significant overall difference in post week 8 BMI across education levels (p = 0.019), so the null hypothesis of equal means was rejected. However, post hoc tests and confidence intervals showed that no specific group comparisons were significant, suggesting that while some variation exists across education levels, the differences are small.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Paired t-test and ANOVA</span>"
    ]
  },
  {
    "objectID": "chapter4.html#hypotheses",
    "href": "chapter4.html#hypotheses",
    "title": "5  t-tests: one sample and two sample",
    "section": "6.1 Hypotheses",
    "text": "6.1 Hypotheses\n\nPhoto by niko nguyen on Unsplash\nSQ1 represents gender, so we can go ahead and rename it to Gender.\nD6_1 represents the average commute time from home to work.\nThe average commuting time in Australia is 29 minutes, according to the Research Report 144 from the Department of Infrastructure and Regional Development. A useful research question to explore from here is whether the average commute from home to work for female carers in NSW is greater than this national benchmark.\nBefore cleaning the data, we need to set out our hypotheses.\n\\(H_0:\\) The average work commute time from home to work for female carers across NSW is not significantly different from 29 mins.\n\\(H_A:\\) The average work commute time from home to work for female carers across NSW was significantly greater than 29 mins.\nWe can express this in mathematical notation:\n\\(H_0:\\) \\(\\mu = 29\\)\n\\(H_A:\\) \\(\\mu &gt; 29\\)\nLet’s prepare the data.\n\necec_1 &lt;- ecec %&gt;%\n  rename(Gender = SQ1,\n        Work_Commute = D6_1) %&gt;%\n  select(Gender, Work_Commute) %&gt;%\n  drop_na(Gender, Work_Commute) \n\necec_1$Gender &lt;- as.factor(ecec_1$Gender)\n\nLet’s take a look at the structure of our new data set.\n\nstr(ecec_1)\n\n'data.frame':   1567 obs. of  2 variables:\n $ Gender      : Factor w/ 3 levels \"Female\",\"Male\",..: 1 1 1 2 2 1 2 1 1 1 ...\n $ Work_Commute: int  65 30 900 40 960 30 34 15 30 15 ...\n\n\nYou might notice that there are 3 levels of Gender. We can check this:\n\ntable(ecec_1$Gender)\n\n\n    Female       Male Non-binary \n       938        626          3 \n\n\nSince we only interested in female carers, we can filter the data.\n\necec_2 &lt;- ecec_1 %&gt;%\n  filter(Gender %in% \"Female\")\n\nLet’s take a look at the structure again.\n\nstr(ecec_2)\n\n'data.frame':   938 obs. of  2 variables:\n $ Gender      : Factor w/ 3 levels \"Female\",\"Male\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Work_Commute: int  65 30 900 30 15 30 15 20 45 30 ...\n\n\nDon’t worry that it says 3 levels. The data has been filtered (we can tell by comparing the observations from before filtering and after).\n\nHow many observations are there? \nHow many variables are there? \nHow many categorical variables are there? \nHow many discrete numerical variables are there? \nHow many continuous numerical variables are there? \n\nNow we can move on to checking the conditions. To start, we’ll create a few useful plots of the home-to-work commute data. These will help us assess whether the observations can reasonably be treated as independent and whether the distribution is close enough to normal for our purposes. The normality requirement becomes less strict as the sample size grows. Slight skewness is fine around n = 15, moderate skewness is acceptable around n = 30, and even strong skewness is workable once the sample size exceeds about 60.\n\n\nI need a hint\n\n\nggplot(data=ecec_2, aes(x=Work_Commute))+\n  geom_histogram(bins=12, col=\"darkseagreen4\", fill=\"darkseagreen2\")+\n  theme_bw()+\n  labs(x=\"Commute Time From Home to Work (mins)\")\n\n\n\n\n\n\n\n\n\nggplot(data=ecec_2, aes(y=Work_Commute, x=\"\"))+\n  geom_boxplot(fill=\"mediumpurple1\")+\n  theme_bw()+\n  labs(y=\"Commute Time From Home to Work (mins)\")\n\n\n\n\n\n\n\n\n\nggplot(data=ecec_2, aes(sample=Work_Commute))+\n  geom_qq()+ #This creates the points\n  geom_qq_line(col=\"firebrick1\")+ #this creates the line\n  labs(x=\"Theoretical quantiles\", y=\"Sample quantiles\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data=ecec_2, aes(y=Work_Commute, x=\"\"))+\n  geom_boxplot(fill=\"mediumpurple1\")+\n  theme_bw()+\n  labs(y=\"Commute Time From Home to Work (mins)\")+\n  geom_jitter(col=\"mediumpurple4\")\n\n\n\n\n\n\n\n\nWith 938 observations, the sample size is well above the point where normality becomes a concern. The plots show that the data are strongly right-skewed with some very large outliers. With a sample this large, the Central Limit Theorem kicks in, so the mean will still have an approximately normal sampling distribution.\nFor independence, we can lean on a few simple checks. Our data represent 938 female carers in NSW, which is only a tiny fraction of the overall population of female carers in the state, so we’re safely under the 10% guideline.\nBecause this comes from what appears to be a large, population-level survey rather, it’s reasonable to treat each person’s commute time as an independent observation.\n\nNow that we’ve checked conditions, we can proceed with t-test. We can do this with one simple line of code:\n\nt.test(ecec_2$Work_Commute, mu = 29, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  ecec_2$Work_Commute\nt = 2.1482, df = 937, p-value = 0.01598\nalternative hypothesis: true mean is greater than 29\n95 percent confidence interval:\n 29.69916      Inf\nsample estimates:\nmean of x \n  31.9936 \n\n\n\n\nthe t-value (the statistic), the degrees of freedom (which is n-1), and the p-value: these allow us to draw our conclusion.\nthe sample mean.\nand the confidence interval – though for a 1-sided test this isn’t a priority.\n\nThe p-value (\\(t_{937}=2.1482, p=0.0.01598\\)) is less than 0.05, so we have evidence to reject the null hypothesis. Therefore, there is evidence that the mean commute time from home to work for female carers in NSW is significantly greater than 29 minutes.\nNote that if instead the alternative hypothesis had been Ha: μ&lt;29 we would instead have:\n\nt.test(ecec_2$Work_Commute, mu = 29, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  ecec_2$Work_Commute\nt = 2.1482, df = 937, p-value = 0.984\nalternative hypothesis: true mean is less than 29\n95 percent confidence interval:\n     -Inf 34.28805\nsample estimates:\nmean of x \n  31.9936 \n\n\nor if it was Ha: μ≠29 we would have\n\nt.test(ecec_2$Work_Commute, mu = 29, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  ecec_2$Work_Commute\nt = 2.1482, df = 937, p-value = 0.03195\nalternative hypothesis: true mean is not equal to 29\n95 percent confidence interval:\n 29.25878 34.72843\nsample estimates:\nmean of x \n  31.9936 \n\n\nNOTE that you can only calculate the confidence intervals with the two sided t-test. EG alternative=\"two.sided\". If you need the confidence intervals, then you need to run this test regardless of what your alternative hypothesis is. You should use the p-value from the test that corresponds to your alternative hypothesis though.\nTherefore, using the output from the two-sided test, with 95% confidence, the average commute time from home-to-work for female carers across NSW is between 29 minutes and 35 minutes longer than the national average. Because this range does not include zero, it points to a clear difference.\nThis naturally raises other questions. For instance, because we are looking at all of NSW, it’s possible that cities like Sydney are pushing the average up. These are the kinds of patterns you could explore further with the dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>t-tests: one sample and two sample</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "7  Chi-square tests",
    "section": "",
    "text": "Our last set of statistical analysis tests are chi-square tests. These tests are looking at association patterns between either two categorical variables or one categorical variable with specific proportions. \nWhen you are comparing two categorical variables, this is called a chi-square test of independence.\nWhen you are comparing one categorical variable with specific proportions, this is called a chi-square goodness of fit test.\n\n8 Chi-square Goodness of Fit Test - Different Proportions\n\nPhoto by Fitsum Admasu on Unsplash\nFor our Chi-square Goodness of Fit Test, we can refer back to the living_well dataset we utilised in the previous chapter. Recall that baseline data was collected between October 2014 and June 2017 from the Living Well Multicultural—Lifestyle Modification Program. People living in ethnic communities in Queensland who were ≥18 years old, and not underweight were eligible to participate. The Living Well Multicultural–Lifestyle Modification Program is a tailored healthy lifestyle program.\nAccording to the Australian Institute of Health and Welfare, the distribution of BMI for females aged 18 and over as of 2022 are:\n\n2.23% underweight\n36% normal weight\n29.6% overweight\n17.4% obese (class 1)\n8% obese (class 2)\n5.4% obese (class 3)\n\nAs we are given BMI and weight class for each individual, a potential research question we can ask is whether the participation in the Living Well Multicultural Program lead to a post-program BMI distribution that differs from the national BMI distribution of adult Australian females as reported by the AIHW (2022).\nWe can combine Obese class 1, 2, and 3 as we don’t differentiate between these classes in our dataset. This gives us 30.8%. Underweight individuals were not allowed to participate in the program, so we can ignore this for now. As we do not have an underweight class in our dataset (as underweight individuals were ineligible to participate), the remaining national proportions no longer sum to 1.\nTo address this, we simply normalise them by dividing each proportion by the total of the three raw proportions. (If your proportions already sum to 1, this step is not required.)\n\nraw &lt;- c(Normal = 0.36, Overweight = 0.296, Obese = 0.308) \nprop &lt;- raw / sum(raw) \nprop \n\n    Normal Overweight      Obese \n 0.3734440  0.3070539  0.3195021 \n\n\n\n8.0.1 Step 1: Hypotheses\nWe can create our hypotheses:\n\\(H_0:\\) The distribution of BMI categories (normal weight, overweight, obese) among female participants in the Living Well Multicultural Program after completing the program matches the national distribution of adult Australian females who are not underweight (normal = 0.373, overweight = 0.307, obese = 0.320).\n\\(H_A\\): The distribution of BMI categories among female participants after the program differs from the national distribution.\nMathematically, we can write the null hypothesis as:\n\\(H_0: p_{normal}=0.373\\) , \\(p_{overweight}=0.307\\) , \\(p_{obese}=0.320\\)\nWe can’t write \\(H_A\\) as an equation, but note that it means at least one proportion is different to these null proportions.\n\n\n8.0.2 Step 2: Conditions\nWe require independence, and sufficient sample size.\nThe chi-square test requires independent observations and adequate expected counts (\\(\\geq 5\\)). In this study, data were collected from individual participants through face-to-face measurements and structured questionnaires at baseline, week 8, and week 14, administered by trained multicultural health workers.\nEach participant provided their own responses and measurements, so the observations can reasonably be treated as independent at a single time point.\nFor sample size, the expected counts in each BMI category must exceed five. Note - We’ll come back to this in a few minutes!\n\n\n8.0.3 Step 3: Run the test\nWe can now load in the dataset. Take a moment to attempt this step yourself by loading the data and the appropriate packages.\n\nType the code for loading in tidyverse. \nType the code to load in the data set. Name your dataset living_well. Be sure to convert strings to factors. \n\n\n\nI need a hint\n\n\nlibrary(tidyverse)  \nliving_well &lt;- read.csv(\"Data sets/Cleaned_data_pre_post.csv\", \n                   stringsAsFactors = TRUE) \n\n\nHow can we filter the data to represent the relevant time we are interested in (baseline vs post week 8) and females? You can also select the variables we are interested in.\n\nWhat function do we use to filter data? \nWhat function do we use to select certain categories in our data? \n\n\n\nShow code\n\n\npost_only &lt;- living_well |&gt;\n  filter(\n    Time == \"Post week8\",\n    Gender == \"Female\") %&gt;%\n  select(ID, BMI, BMI_cate1, BMI_cate2)\n\n\nWe can now remove any missing values from the relevant columns. In this dataset, the value 999 indicates a missing entry.\n\nWhat function allows us to modify or create variables? \nWhat function converts the value 999 into a missing value? \nWhat function removes rows containing missing values? \n\nNote that we can use droplevels to remove any unused factor levels that remain after filtering.\n\n\nShow code\n\n\npost_only &lt;- post_only %&gt;%\n  mutate(\n    BMI_cate1 = na_if(BMI_cate1, \"999\"),\n    BMI_cate2 = na_if(BMI_cate2, \"999\")\n  ) %&gt;%\n  drop_na(BMI_cate1, BMI_cate2) %&gt;%\n  droplevels()\n\n\nNext, we create a table of observed counts for each BMI category in the dataset and relevel the factor so that the category order matches the order of the expected proportions.\n\nWhat function creates a table? \nWhat order should our labels be in? Please list the labels separated by commas (for example: x, y, z . \n\n\n\nShow code\n\n\ncounts &lt;- table(factor(\n  post_only$BMI_cate1,\n  levels = c(\"Normal weight\", \"Overweight\", \"Obese\")\n))\ncounts\n\n\nNormal weight    Overweight         Obese \n          101           132           159 \n\n\n\nWe now need to ensure that the expected proportions are in the same order as the BMI category levels in our dataset.\nWe can now run the chi-square goodness of fit test.\n\nChiT &lt;- chisq.test(counts, p = prop)\nChiT\n\n\n    Chi-squared test for given probabilities\n\ndata:  counts\nX-squared = 24.296, df = 2, p-value = 5.299e-06\n\n\nBased on the output above:\n\nWhat is the degrees of freedom? \nWhat is the test statistic? \nShould we accept or reject the null hypothesis? \n\nUsing the output shown, try writing a conclusion.\n\n\nShow conclusion\n\nOur output (\\(\\chi^2 =24.296\\), \\(df=2\\), \\(p = 5.299*10^{-6}\\)) provides us with enough evidence to reject the null hypothesis and conclude that the BMI distribution of female participants after completing the program is different from the stated distribution.\n\nA couple of notes:\n\nWe are able to make this conclusion as the p-value is less than 0.05\nThe df (degrees of freedom) in this case is calculated as the number of categories minus 1. Given that there are three different BMI categories that are being examined the df = 3 - 1 = 2.\nRStudio can calculate the expected counts for you:\n\n\nChiT$expected\n\nNormal weight    Overweight         Obese \n     146.3900      120.3651      125.2448 \n\n\nAs the expected counts are much greater than 5, this tells us that the conditions for the chi-square goodness of fit test has been satisfied.\nIf you were interested in which BMI categories contributed to the significant result you could use:\n\nChiT$stdres\n\n\nNormal weight    Overweight         Obese \n    -4.739416      1.273975      3.656346 \n\n\nThis command will calculate standardised residual values for each category.\nAs a rule of thumb, a standard residual value greater than 2 indicates that counts for that given category were higher than would be expected if the given proportions being tested was true.\nA standard residual value less than -2 indicates that counts for that given category were lower than would be expected if the given proportions being tested was true.\n\n\n\n9 Chi-square Test of Independence\n\nPhoto by Towfiqu barbhuiya on Unsplash\nWe will return to the 2023 Early Childhood Education and Care (ECEC) survey, which we first examined when learning about t-tests. This survey gathered responses from roughly 2,000 NSW parents and carers during January and February 2023, and aimed to understand which policy options families value most and what barriers influence their access to early childhood education and care.\nBegin by loading the dataset.\n\nLoad in the library ggmosaic. \nLoad in the library forcats. \nType the code to load in the data set. Name your dataset ecec. Be sure to convert strings to factors. \n\n\n\nShow code\n\n\nlibrary(forcats)\nlibrary(ggmosaic)\n\necec &lt;- read.csv(\"Data sets/ecec-survey-2023.csv\",\n                 stringsAsFactors = TRUE) \n\n\nTake some time to think of research questions you might ask about this dataset where two categorical variables are compared.\nFor this section, we will focus on the research question: Does the distribution of income categories differ between male and female carers?\n\n9.0.1 1. Step 1: Hypotheses\n\\(H_0:\\) Gender and income level for carers across NSW is independent.\n\\(H_A:\\) Gender and income level for carers across NSW are associated.\n(we typically wouldn’t write these hypotheses as equations)\n\n\n9.0.2 2. Step 2: Conditions\nWhat conditions do we need to meet for a chi-square test?\n\n\nShow conditions\n\n\nindependence\nexpected count greater than 5\n\n\nTry cleaning the dataset yourself. Filter the variables you need, rename any that will make the analysis clearer, and drop any unused factor levels.\n\n\nShow code\n\n\necec_chi &lt;- ecec %&gt;%\n  rename(\n    Gender = SQ1,\n    Annual_Income = D13\n  ) %&gt;%\n  select(Gender, Annual_Income) %&gt;%\n  drop_na(Gender, Annual_Income) %&gt;%\n  filter(Gender %in% c(\"Male\", \"Female\")) %&gt;%\n  filter(Annual_Income != \"Prefer not to answer\") %&gt;%\n  droplevels() \n\n\nstr(ecec_chi)\n\n'data.frame':   1924 obs. of  2 variables:\n $ Gender       : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 1 1 2 1 2 2 1 ...\n $ Annual_Income: Factor w/ 14 levels \"$1 - $19,999 per year ($1 - $379 per week)\",..: 5 2 3 2 2 13 5 4 2 4 ...\n\n\n\nWe can use a mosaic plot to visualise the relative proportions across groups. Because some of the labels are quite long, renaming them to more concise terms will help improve the readability of the plot.\n\necec_chi3 &lt;- ecec_chi %&gt;%\n  mutate(Income_Group = fct_collapse(\n    Annual_Income,\n    `Zero/Neg` = \"Negative or Zero Income\",\n    `&lt;50k` = c(\"$1 - $19,999 per year ($1 - $379 per week)\",\n               \"$20,000 - $29,999 per year ($380 - $579 per week)\",\n               \"$30,000 - $39,999 per year ($580 - $769 per week)\",\n               \"$40,000 - $49,999 per year ($770 - $959 per week)\"),\n    `50k-100k` = c(\"$50,000 - $59,999 per year ($960 - $1149 per week)\",\n                   \"$60,000 - $79,999 per year ($1150 - $1529 per week)\",\n                   \"$80,000 - $99,999 per year ($1530 - $1919 per week)\"),\n    `100k-150k` = c(\"$100,000 - $124,999 per year ($1920 - $2399 per week)\",\n                    \"$125,000 - $149,999 per year ($2400 - $2879 per week)\"),\n    `150k+` = c(\"$150,000 - $199,999 per year ($2880 - $3839 per week)\",\n                \"$200,000 - $249,999 per year ($3840 - $4799 per week)\",\n                \"$250,000 - $299,999 per year ($4800 - $5759 per week)\",\n                \"$300,000 or more per year ($5760 or more per week)\")\n  ))\n\nggplot(ecec_chi3) +\n  geom_mosaic(aes(x = product(Income_Group), \n                  fill = Gender, \n                  weight = 1),\n              color = \"white\", size = 0.2) +\n  theme_minimal() +\n  labs(title = \"Gender by Income Group\",\n       x = \"Income Group\",\n      y = \"Gender\")\n\n\n\n\n\n\n\n\nHow would you interpret the above plot?\n\n\nShow interpretation\n\nClearly, there are a very small number of observations for our Zero/Negative income category (Try checking this - you should only see 3!). This will likely cause an issue once we run the chi-square test, as the expected count won’t be above 5.\nIt appears that 50k-100k and 100-150k are the most common income groups, followed by 150k+ and then &lt;50k for both men and women. We can see that there are many more women who earn under 50k than men. In the 150k+ income group, the proportion of men and women is fairly even.\nIf we take a look at the number of females and males in our study:\n\ntable(ecec_chi$Gender)\n\n\nFemale   Male \n  1249    675 \n\n\nThere are nearly double the number of females compared with males, which means the plot will naturally show more pink overall.\n\n\n\n9.0.3 3. Step 3: Run the test\nNow try running the chi-square test.\n\nChiT_ecec &lt;- chisq.test(table(ecec_chi$Annual_Income,ecec_chi$Gender))\n\nWarning in chisq.test(table(ecec_chi$Annual_Income, ecec_chi$Gender)):\nChi-squared approximation may be incorrect\n\n\nYou may notice a warning. This typically appears when one of the expected counts is below 5.\n\nChiT_ecec$expected\n\n                                                       \n                                                            Female       Male\n  $1 - $19,999 per year ($1 - $379 per week)             24.668399  13.331601\n  $100,000 - $124,999 per year ($1920 - $2399 per week) 212.927235 115.072765\n  $125,000 - $149,999 per year ($2400 - $2879 per week) 148.010395  79.989605\n  $150,000 - $199,999 per year ($2880 - $3839 per week) 195.399688 105.600312\n  $20,000 - $29,999 per year ($380 - $579 per week)      45.441788  24.558212\n  $200,000 - $249,999 per year ($3840 - $4799 per week)  75.303534  40.696466\n  $250,000 - $299,999 per year ($4800 - $5759 per week)  20.773389  11.226611\n  $30,000 - $39,999 per year ($580 - $769 per week)      46.740125  25.259875\n  $300,000 or more per year ($5760 or more per week)     18.176715   9.823285\n  $40,000 - $49,999 per year ($770 - $959 per week)      46.090956  24.909044\n  $50,000 - $59,999 per year ($960 - $1149 per week)     70.110187  37.889813\n  $60,000 - $79,999 per year ($1150 - $1529 per week)   141.518711  76.481289\n  $80,000 - $99,999 per year ($1530 - $1919 per week)   201.891372 109.108628\n  Negative or Zero Income                                 1.947505   1.052495\n\n\nIn this case, the expected count for the ‘Negative or Zero Income’ category is 1.95 and 1.05, which is well under our condition of \\(\\geq 5\\). To address this, we can combine this category with the next lowest group, ‘$1–19,999 per year’, and create a new category labelled ‘under 20k’.\n\necec_chi &lt;- ecec_chi %&gt;%\n  mutate(Annual_Income = fct_collapse(\n    Annual_Income,\n    `Under $20k` = c(\"Negative or Zero Income\",\n                     \"$1 - $19,999 per year ($1 - $379 per week)\")\n  ))\n\nAfter combining the categories, run the test again.\n\nChiT_ecec &lt;- chisq.test(table(ecec_chi$Annual_Income,ecec_chi$Gender))\nChiT_ecec\n\n\n    Pearson's Chi-squared test\n\ndata:  table(ecec_chi$Annual_Income, ecec_chi$Gender)\nX-squared = 54.962, df = 12, p-value = 1.838e-07\n\n\nVoila! No warnings are produced, indicating that the chi-square assumptions are satisfied. You can check the expected counts to verify.\n\nChiT_ecec$expected\n\n                                                       \n                                                           Female       Male\n  Under $20k                                             26.61590  14.384096\n  $100,000 - $124,999 per year ($1920 - $2399 per week) 212.92723 115.072765\n  $125,000 - $149,999 per year ($2400 - $2879 per week) 148.01040  79.989605\n  $150,000 - $199,999 per year ($2880 - $3839 per week) 195.39969 105.600312\n  $20,000 - $29,999 per year ($380 - $579 per week)      45.44179  24.558212\n  $200,000 - $249,999 per year ($3840 - $4799 per week)  75.30353  40.696466\n  $250,000 - $299,999 per year ($4800 - $5759 per week)  20.77339  11.226611\n  $30,000 - $39,999 per year ($580 - $769 per week)      46.74012  25.259875\n  $300,000 or more per year ($5760 or more per week)     18.17672   9.823285\n  $40,000 - $49,999 per year ($770 - $959 per week)      46.09096  24.909044\n  $50,000 - $59,999 per year ($960 - $1149 per week)     70.11019  37.889813\n  $60,000 - $79,999 per year ($1150 - $1529 per week)   141.51871  76.481289\n  $80,000 - $99,999 per year ($1530 - $1919 per week)   201.89137 109.108628\n\n\nNow that the chi-square assumptions have been met and the test is valid, try interpreting the output. Look at the p-value and decide whether to reject or not reject the null hypothesis, and explain what this result means in the context of the research question.\n\n\nShow conclusion\n\nOur output (\\(\\chi^2 =54.962\\), \\(df=12\\), \\(p = 1.838*10^{-7}\\)) provides us with enough evidence to reject the null hypothesis and conclude that the annual income and gender are associated for carers across NSW.\n\nYou can also take a look at the standardised residuals.\n\nChiT_ecec$stdres\n\n                                                       \n                                                             Female        Male\n  Under $20k                                             2.77340715 -2.77340715\n  $100,000 - $124,999 per year ($1920 - $2399 per week) -1.76923708  1.76923708\n  $125,000 - $149,999 per year ($2400 - $2879 per week) -0.44495658  0.44495658\n  $150,000 - $199,999 per year ($2880 - $3839 per week) -2.81409940  2.81409940\n  $20,000 - $29,999 per year ($380 - $579 per week)      2.69377297 -2.69377297\n  $200,000 - $249,999 per year ($3840 - $4799 per week) -1.06441668  1.06441668\n  $250,000 - $299,999 per year ($4800 - $5759 per week)  0.08464871 -0.08464871\n  $30,000 - $39,999 per year ($580 - $769 per week)      3.84095076 -3.84095076\n  $300,000 or more per year ($5760 or more per week)    -2.46396134  2.46396134\n  $40,000 - $49,999 per year ($770 - $959 per week)      3.01775862 -3.01775862\n  $50,000 - $59,999 per year ($960 - $1149 per week)    -0.02286838  0.02286838\n  $60,000 - $79,999 per year ($1150 - $1529 per week)    0.97682805 -0.97682805\n  $80,000 - $99,999 per year ($1530 - $1919 per week)   -0.37521579  0.37521579",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chi-square tests</span>"
    ]
  },
  {
    "objectID": "experimental.html",
    "href": "experimental.html",
    "title": "8  Chi-square tests",
    "section": "",
    "text": "Our last set of statistical analysis tests are chi-square tests. These tests are looking at association patterns between either two categorical variables or one categorical variable with specific proportions. \nWhen you are comparing two categorical variables, this is called a chi-square test of independence.\nWhen you are comparing one categorical variable with specific proportions, this is called a chi-square goodness of fit test.\n\n9 Chi-square Goodness of Fit Test - Different Proportions\nFor our Chi-square Goodness of Fit Test, we can refer back to the living_well dataset we utilised in the previous chapter. Recall that baseline data was collected between October 2014 and June 2017 from the Living Well Multicultural—Lifestyle Modification Program. People living in ethnic communities in Queensland who were ≥18 years old, and not underweight were eligible to participate. The Living Well Multicultural–Lifestyle Modification Program is a tailored healthy lifestyle program.\nAccording to the Australian Institute of Health and Welfare, the distribution of BMI for females aged 18 and over as of 2022 are:\n\n2.23% underweight\n36% normal weight\n29.6% overweight\n17.4% obese (class 1)\n8% obese (class 2)\n5.4% obese (class 3)\n\nAs we are given BMI and weight class for each individual, a potential research question we can ask is whether the participation in the Living Well Multicultural Program lead to a post-program BMI distribution that differs from the national BMI distribution of adult Australian females as reported by the AIHW (2022).\nWe can combine Obese class 1, 2, and 3 as we don’t differentiate between these classes in our dataset. This gives us 30.8%. Underweight individuals were not allowed to participate in the program, so we can ignore this for now. As we do not have an underweight class in our dataset (as underweight individuals were ineligible to participate), we can filter this out of the dataset.\n\n9.0.1 Step 1: Hypotheses\nWe can create our hypotheses:\n\\(H_0:\\) The distribution of BMI categories (normal weight, overweight, obese) among female participants in the Living Well Multicultural Program after completing the program matches the national distribution of adult Australian females (normal = 0.36, overweight = 0.296, obese = 0.308).\n\\(H_A\\): The distribution of BMI categories among female participants after the program differs from the national distribution.\nMathematically, we can write the null hypothesis as:\n\\(H_0: p_{normal}=0.36\\) , \\(p_{overweight}=0.296\\) , \\(p_{obese}=0.308\\)\nWe can’t write \\(H_A\\) as an equation, but note that it means at least one proportion is different to these null proportions.\n\n\n9.0.2 Step 2: Conditions\nWe require independence, and sufficient sample size.\n\nWe will assume that random sampling was used and thus the observations are independent\nSample size: As long as the expected values for each count are greater than 5 we’re okay. As we’re looking at different proportions for this question you’ll need to take this into account. Note - We’ll come back to this in a few minutes!\n\n\n\n9.0.3 Step 3: Run the test\nWe can now load in the dataset. Take a moment to attempt this step yourself by loading the data and the appropriate packages.\n\n\n\n\n\n\n\n\n\n\n\n# Load in tidyverse package\nlibrary(tidyverse)\n# Load in ggmosaic package\nlibrary(ggmosaic)\n# Load in data set \n\nliving_well &lt;- read.csv(\"Data sets/Cleaned_data_pre_post.csv\", \n                   stringsAsFactors = TRUE\n\n# Load in tidyverse package\nlibrary(tidyverse)\n# Load in ggmosaic package\nlibrary(ggmosaic)\n# Load in data set \n\nliving_well &lt;- read.csv(\"Data sets/Cleaned_data_pre_post.csv\", \n                   stringsAsFactors = TRUE\n\n\n\n\n\n\nlibrary(ggplot2) \nlibrary(tidyverse)\nlibrary(ggmosaic)\nliving_well &lt;- read.csv(\"Data sets/Cleaned_data_pre_post.csv\", \n                   stringsAsFactors = TRUE) \n\nHow can we filter the data to represent the relevant time we are interested in (baseline vs post week 8) and females? You can also select the variables we are interested in.\n\npost_only &lt;- living_well |&gt;\n  filter(\n    Time == \"Post week8\",\n    Gender == \"Female\") %&gt;%\n  select(ID, BMI, BMI_cate1, BMI_cate2)\n\nWe can now drop any NA values in the relevant columns. How can we do this?\n\n\n\n\n\n\n\n\n\n\npost_only &lt;- post_only %&gt;%\n  mutate(\n    BMI_cate1 = na_if(BMI_cate1, \"999\"),\n    BMI_cate2 = na_if(BMI_cate2, \"999\")\n  ) %&gt;%\n  drop_na(BMI_cate1, BMI_cate2) %&gt;%\n  droplevels()\npost_only &lt;- post_only %&gt;%\n  mutate(\n    BMI_cate1 = na_if(BMI_cate1, \"999\"),\n    BMI_cate2 = na_if(BMI_cate2, \"999\")\n  ) %&gt;%\n  drop_na(BMI_cate1, BMI_cate2) %&gt;%\n  droplevels()\n\n\n\n\n\n\npost_only &lt;- post_only %&gt;%\n  mutate(\n    BMI_cate1 = na_if(BMI_cate1, \"999\"),\n    BMI_cate2 = na_if(BMI_cate2, \"999\")\n  ) %&gt;%\n  drop_na(BMI_cate1, BMI_cate2) %&gt;%\n  droplevels()\n\nNext, we create a table of observed counts for each BMI category in the dataset and relevel the factor so that the category order matches the order of the expected proportions.\n\ncounts &lt;- table(factor(\n  post_only$BMI_cate1,\n  levels = c(\"Normal weight\", \"Overweight\", \"Obese\")\n))\ncounts\n\n\nNormal weight    Overweight         Obese \n          101           132           159 \n\n\nWe now need to ensure that the expected proportions are in the same order as the BMI category levels in our dataset.\nBecause the underweight category was removed, the remaining national proportions no longer sum to 1. To address this, we simply normalise them by dividing each proportion by the total of the three raw proportions. (If your proportions already sum to 1, this step is not required.)\n\nraw &lt;- c(Normal = 0.36, Overweight = 0.296, Obese = 0.308)\nprop &lt;- raw / sum(raw)\nprop\n\n    Normal Overweight      Obese \n 0.3734440  0.3070539  0.3195021 \n\n\nWe can now run the chi-square goodness of fit test.\n\nChiT &lt;- chisq.test(counts, p = prop)\nChiT\n\n\n    Chi-squared test for given probabilities\n\ndata:  counts\nX-squared = 24.296, df = 2, p-value = 5.299e-06\n\n\nOur output (\\(\\chi^2 =24.296\\), \\(df=2\\), \\(p = 5.299*10^{-6}\\)) provides us with enough evidence to reject the null hypothesis and conclude that the BMI distribution of female participants after completing the program is different from the stated distribution.\nA couple of notes:\n\nWe are able to make this conclusion as the p-value is less than 0.05\nThe df (degrees of freedom) in this case is calculated as the number of categories minus 1. Given that there are three different BMI categories that are being examined the df = 3 - 1 = 2.\nRStudio can calculate the expected counts for you:\n\n\nChiT$expected\n\nNormal weight    Overweight         Obese \n     146.3900      120.3651      125.2448 \n\n\nAs the expected counts are much greater than 5, this tells us that the conditions for the chi-square goodness of fit test has been satisfied.\nIf you were interested in which BMI categories contributed to the significant result you could use:\n\nChiT$stdres\n\n\nNormal weight    Overweight         Obese \n    -4.739416      1.273975      3.656346 \n\n\nThis command will calculate standardised residual values for each category.\nAs a rule of thumb, a standard residual value greater than 2 indicates that counts for that given category were higher than would be expected if the given proportions being tested was true.\nA standard residual value less than -2 indicates that counts for that given category were lower than would be expected if the given proportions being tested was true.\n\n\n\n10 Chi-square Test of Independence\nWe will return to the 2023 Early Childhood Education and Care (ECEC) survey, which we first examined when learning about t-tests. This survey gathered responses from roughly 2,000 NSW parents and carers during January and February 2023, and aimed to understand which policy options families value most and what barriers influence their access to early childhood education and care.\n\nBegin by loading the dataset.\n\nlibrary(dplyr) \nlibrary(tidyr) \nlibrary(ggplot2)  \nlibrary(forcats)\nlibrary(ggmosaic)\necec &lt;- read.csv(\"Data sets/ecec-survey-2023.csv\",\n                 stringsAsFactors = TRUE) \n\nA possible research question is: Does the distribution of income categories differ between male and female carers?\n\n10.0.1 1. Hypotheses\n\\(H_0:\\) Gender and income level for carers across NSW is independent.\n\\(H_A:\\) Gender and income level for carers across NSW are associated.\n(we typically wouldn’t write these hypotheses as equations)\n\n\n10.0.2 2. Conditions\n\nindependence\nexpected count greater than 5\n\nTry cleaning the dataset yourself. Filter the variables you need, rename any that will make the analysis clearer, and drop any unused factor levels.\n\necec_chi &lt;- ecec %&gt;%\n  rename(\n    Gender = SQ1,\n    Annual_Income = D13\n  ) %&gt;%\n  select(Gender, Annual_Income) %&gt;%\n  drop_na(Gender, Annual_Income) %&gt;%\n  filter(Gender %in% c(\"Male\", \"Female\")) %&gt;%\n  filter(Annual_Income != \"Prefer not to answer\") %&gt;%\n  droplevels() \n\n\nstr(ecec_chi)\n\n'data.frame':   1924 obs. of  2 variables:\n $ Gender       : Factor w/ 2 levels \"Female\",\"Male\": 2 2 1 1 1 2 1 2 2 1 ...\n $ Annual_Income: Factor w/ 14 levels \"$1 - $19,999 per year ($1 - $379 per week)\",..: 5 2 3 2 2 13 5 4 2 4 ...\n\n\nWe can use a mosaic plot to visualise the relative proportions across groups. Because some of the labels are quite long, renaming them to more concise terms will help improve the readability of the plot.\n\necec_chi3 &lt;- ecec_chi %&gt;%\n  mutate(Income_Group = fct_collapse(\n    Annual_Income,\n    `Zero/Neg` = \"Negative or Zero Income\",\n    `&lt;50k` = c(\"$1 - $19,999 per year ($1 - $379 per week)\",\n               \"$20,000 - $29,999 per year ($380 - $579 per week)\",\n               \"$30,000 - $39,999 per year ($580 - $769 per week)\",\n               \"$40,000 - $49,999 per year ($770 - $959 per week)\"),\n    `50k-100k` = c(\"$50,000 - $59,999 per year ($960 - $1149 per week)\",\n                   \"$60,000 - $79,999 per year ($1150 - $1529 per week)\",\n                   \"$80,000 - $99,999 per year ($1530 - $1919 per week)\"),\n    `100k-150k` = c(\"$100,000 - $124,999 per year ($1920 - $2399 per week)\",\n                    \"$125,000 - $149,999 per year ($2400 - $2879 per week)\"),\n    `150k+` = c(\"$150,000 - $199,999 per year ($2880 - $3839 per week)\",\n                \"$200,000 - $249,999 per year ($3840 - $4799 per week)\",\n                \"$250,000 - $299,999 per year ($4800 - $5759 per week)\",\n                \"$300,000 or more per year ($5760 or more per week)\")\n  ))\n\nggplot(ecec_chi3) +\n  geom_mosaic(aes(x = product(Income_Group), \n                  fill = Gender, \n                  weight = 1),\n              color = \"white\", size = 0.2) +\n  theme_minimal() +\n  labs(title = \"Gender by Income Group\",\n       x = \"Income Group\",\n      y = \"Gender\")\n\n\n\n\n\n\n\n\n\n\n10.0.3 3. Run the test\nNow try running the chi-square test.\n\nChiT_ecec &lt;- chisq.test(table(ecec_chi$Annual_Income,ecec_chi$Gender))\n\nYou may notice a warning. This typically appears when one of the expected counts is below 5.\n\nChiT_ecec$expected\n\n                                                       \n                                                            Female       Male\n  $1 - $19,999 per year ($1 - $379 per week)             24.668399  13.331601\n  $100,000 - $124,999 per year ($1920 - $2399 per week) 212.927235 115.072765\n  $125,000 - $149,999 per year ($2400 - $2879 per week) 148.010395  79.989605\n  $150,000 - $199,999 per year ($2880 - $3839 per week) 195.399688 105.600312\n  $20,000 - $29,999 per year ($380 - $579 per week)      45.441788  24.558212\n  $200,000 - $249,999 per year ($3840 - $4799 per week)  75.303534  40.696466\n  $250,000 - $299,999 per year ($4800 - $5759 per week)  20.773389  11.226611\n  $30,000 - $39,999 per year ($580 - $769 per week)      46.740125  25.259875\n  $300,000 or more per year ($5760 or more per week)     18.176715   9.823285\n  $40,000 - $49,999 per year ($770 - $959 per week)      46.090956  24.909044\n  $50,000 - $59,999 per year ($960 - $1149 per week)     70.110187  37.889813\n  $60,000 - $79,999 per year ($1150 - $1529 per week)   141.518711  76.481289\n  $80,000 - $99,999 per year ($1530 - $1919 per week)   201.891372 109.108628\n  Negative or Zero Income                                 1.947505   1.052495\n\n\nIn this case, the expected count for the ‘Negative or Zero Income’ category is 1.95 and 1.05, which is well under our condition of &gt;5. To address this, we can combine this category with the next lowest group, ‘$1–19,999 per year’, and create a new category labelled ‘under 20k’.\n\necec_chi &lt;- ecec_chi %&gt;%\n  mutate(Annual_Income = fct_collapse(\n    Annual_Income,\n    `Under $20k` = c(\"Negative or Zero Income\",\n                     \"$1 - $19,999 per year ($1 - $379 per week)\")\n  ))\n\nAfter combining the categories, run the test again.\n\nChiT_ecec &lt;- chisq.test(table(ecec_chi$Annual_Income,ecec_chi$Gender))\nChiT_ecec\n\n\n    Pearson's Chi-squared test\n\ndata:  table(ecec_chi$Annual_Income, ecec_chi$Gender)\nX-squared = 54.962, df = 12, p-value = 1.838e-07\n\n\nVoila! No warnings are produced, indicating that the chi-square assumptions are satisfied. You can check the expected counts to verify.\n\nChiT_ecec$expected\n\n                                                       \n                                                           Female       Male\n  Under $20k                                             26.61590  14.384096\n  $100,000 - $124,999 per year ($1920 - $2399 per week) 212.92723 115.072765\n  $125,000 - $149,999 per year ($2400 - $2879 per week) 148.01040  79.989605\n  $150,000 - $199,999 per year ($2880 - $3839 per week) 195.39969 105.600312\n  $20,000 - $29,999 per year ($380 - $579 per week)      45.44179  24.558212\n  $200,000 - $249,999 per year ($3840 - $4799 per week)  75.30353  40.696466\n  $250,000 - $299,999 per year ($4800 - $5759 per week)  20.77339  11.226611\n  $30,000 - $39,999 per year ($580 - $769 per week)      46.74012  25.259875\n  $300,000 or more per year ($5760 or more per week)     18.17672   9.823285\n  $40,000 - $49,999 per year ($770 - $959 per week)      46.09096  24.909044\n  $50,000 - $59,999 per year ($960 - $1149 per week)     70.11019  37.889813\n  $60,000 - $79,999 per year ($1150 - $1529 per week)   141.51871  76.481289\n  $80,000 - $99,999 per year ($1530 - $1919 per week)   201.89137 109.108628\n\n\nNow that the chi-square assumptions have been met and the test is valid, try interpreting the output. Look at the p-value and decide whether to reject or not reject the null hypothesis, and explain what this result means in the context of the research question.\n%%%%%%%%%%%%%%%%%%%%%%\nYou can also take a look at the standardised residuals.\n\nChiT_ecec$stdres\n\n                                                       \n                                                             Female        Male\n  Under $20k                                             2.77340715 -2.77340715\n  $100,000 - $124,999 per year ($1920 - $2399 per week) -1.76923708  1.76923708\n  $125,000 - $149,999 per year ($2400 - $2879 per week) -0.44495658  0.44495658\n  $150,000 - $199,999 per year ($2880 - $3839 per week) -2.81409940  2.81409940\n  $20,000 - $29,999 per year ($380 - $579 per week)      2.69377297 -2.69377297\n  $200,000 - $249,999 per year ($3840 - $4799 per week) -1.06441668  1.06441668\n  $250,000 - $299,999 per year ($4800 - $5759 per week)  0.08464871 -0.08464871\n  $30,000 - $39,999 per year ($580 - $769 per week)      3.84095076 -3.84095076\n  $300,000 or more per year ($5760 or more per week)    -2.46396134  2.46396134\n  $40,000 - $49,999 per year ($770 - $959 per week)      3.01775862 -3.01775862\n  $50,000 - $59,999 per year ($960 - $1149 per week)    -0.02286838  0.02286838\n  $60,000 - $79,999 per year ($1150 - $1529 per week)    0.97682805 -0.97682805\n  $80,000 - $99,999 per year ($1530 - $1919 per week)   -0.37521579  0.37521579",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chi-square tests</span>"
    ]
  }
]